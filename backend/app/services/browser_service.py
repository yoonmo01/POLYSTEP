# app/services/browser_service.py

import asyncio
import json
import logging
import inspect
import os
import re
import base64
import sys
import time
from collections import deque
from datetime import datetime, timezone
from urllib.parse import urlparse, urljoin
from typing import Any, Dict, List, Optional, Callable, Awaitable, Set, Tuple

import httpx
from dotenv import load_dotenv, find_dotenv

from browser_use import Agent, Browser, ChatGoogle

# âœ… (ì¶”ê°€) JSONì´ ì•„ë‹Œ í…ìŠ¤íŠ¸ ìµœì¢…ê²°ê³¼(ì •ì±… ëª» ì°¾ìŒ ë“±) ë³µêµ¬ìš© íœ´ë¦¬ìŠ¤í‹±
_NOT_FOUND_HINTS = [
    "could not be found",
    "cannot be found",
    "not be found",
    "no matching data",
    "no data available",
    "ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤",
    "ì¡°íšŒëœ",
    "ì—†ìŠµë‹ˆë‹¤",
    # âœ… í•œêµ­ì–´ ì‹¤íŒ¨/ë¯¸ë°œê²¬ ì¼€ì´ìŠ¤(LLMì´ ìì—°ì–´ë¡œ ì‹¤íŒ¨ ìš”ì•½í•  ë•Œ ëŒ€ë¹„)
    "ì°¾ì„ ìˆ˜ ì—†",         # ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤/ì—†ìŒ ë“±
    "ë°œê²¬í•˜ì§€ ëª»",        # ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤
    "í™•ì¸í•  ìˆ˜ ì—†",       # í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤
    "ì •ë³´ê°€ ì—†",          # ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤
    "í˜ì´ì§€ê°€ ë¹„ì–´",      # í˜ì´ì§€ê°€ ë¹„ì–´ ìˆì–´
    "ìƒì„¸ ì •ë³´ë¥¼",        # ìƒì„¸ ì •ë³´ë¥¼ ë‹´ì€ ... ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (ê°™ì€ íŒ¨í„´)
]
_JUDGE_FAIL_HINTS = [
    "Judge Verdict: âŒ FAIL",
    "Failure Reason:",
    "âš–ï¸  Judge Verdict",
]

# âœ… browser-use ë²„ì „ì— ë”°ë¼ Controllerê°€ ì—†ì„ ìˆ˜ ìˆìŒ
try:
    from browser_use import Controller  # type: ignore
except Exception:
    Controller = None  # type: ignore

from app.utils.file_utils import ensure_download_dir
from app.models import Policy

logger = logging.getLogger(__name__)

# âœ… Windowsì—ì„œ subprocessë¥¼ ì“°ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬(browser-use/Playwright) ì•ˆì „ì¥ì¹˜
_IS_WINDOWS = sys.platform.startswith("win")
_IS_LINUX = sys.platform.startswith("linux")

# (ì˜µì…˜) import ì‹œì ì—ë„ policyë¥¼ í•œ ë²ˆ ì„¸íŒ…í•´ë‘”ë‹¤ (ì´ë¯¸ ìƒì„±ëœ loopì—ëŠ” ì˜í–¥ ì—†ê³ , ìƒˆ loopì—ë§Œ ì ìš©)
if _IS_WINDOWS:
    try:
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
        logger.info("[BrowserService] (import) set WindowsProactorEventLoopPolicy")
    except Exception as e:
        logger.warning("[BrowserService] (import) failed to set Proactor policy: %s", e)

# âœ… .env ë¡œë”©ì„ "í™•ì‹¤í•˜ê²Œ"
_DOTENV_PATH = find_dotenv(".env", usecwd=True)
_DOTENV_OVERRIDE = os.getenv("DOTENV_OVERRIDE", "false").lower() in (
    "1",
    "true",
    "yes",
    "y",
    "on",
)
load_dotenv(_DOTENV_PATH, override=_DOTENV_OVERRIDE)
logger.info(
    "[BrowserService] dotenv loaded from: %s (override=%s)",
    _DOTENV_PATH or "(not found)",
    _DOTENV_OVERRIDE,
)

# WebSocket ìª½ì—ì„œëŠ” async ì½œë°±ì„ ì“¸ ìˆ˜ ìˆìœ¼ë‹ˆê¹Œ ì´ë ‡ê²Œ íƒ€ì… ì •ì˜
AsyncLogCallback = Optional[Callable[[str], Awaitable[None]]]
AsyncScreenshotCallback = Optional[Callable[[str], Awaitable[None]]]

_SCREENSHOT_TIMEOUT_RE = re.compile(
    r"ScreenshotWatchdog\.on_ScreenshotEvent.*timed out", re.IGNORECASE
)
_DOMWATCHDOG_SCREENSHOT_FAIL_RE = re.compile(r"Clean screenshot failed", re.IGNORECASE)

# âœ… Browser ì‹œì‘/ëŸ°ì¹˜(CDP) íƒ€ì„ì•„ì›ƒ ê³„ì—´ íŒ¨í„´ (ì´ë²ˆ AWS ì—ëŸ¬ ëŒ€ì‘)
_BROWSER_START_TIMEOUT_RE = re.compile(
    r"(BrowserStartEvent|BrowserLaunchEvent).*timed out|Cannot connect to host 127\.0\.0\.1:\d+|_wait_for_cdp_url",
    re.IGNORECASE,
)


def _is_browser_snapshot_timeout(e: Exception) -> bool:
    msg = f"{type(e).__name__}: {e}"
    return bool(
        _SCREENSHOT_TIMEOUT_RE.search(msg) or _DOMWATCHDOG_SCREENSHOT_FAIL_RE.search(msg)
    )


def _is_browser_start_timeout(e: Exception) -> bool:
    msg = f"{type(e).__name__}: {e}"
    return bool(_BROWSER_START_TIMEOUT_RE.search(msg))


def _normalize_url(raw: Optional[str]) -> str:
    """
    âœ… ì •ì±…/DBì— 'www.xxx.com' ì²˜ëŸ¼ scheme ì—†ëŠ” URLì´ ë“¤ì–´ì˜¤ëŠ” ì¼€ì´ìŠ¤ ì •ê·œí™”.
    """
    s = (raw or "").strip()
    if not s:
        return ""
    s = s.replace(" ", "").replace("\n", "").replace("\r", "")
    if not s.startswith(("http://", "https://")):
        s = "https://" + s
    try:
        p = urlparse(s)
        if not p.netloc:
            return s
    except Exception:
        return s
    return s


def _env_flag(name: str, default: str = "false") -> bool:
    return os.getenv(name, default).lower() in ("1", "true", "yes", "y", "on")


def _env_int(name: str, default: int = 0) -> int:
    try:
        return int(os.getenv(name, str(default)))
    except Exception:
        return default


def _build_kwargs_for_callable(fn: Any, desired: Dict[str, Any]) -> Dict[str, Any]:
    """
    âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ ì°¨ì´ë¡œ __init__ ì‹œê·¸ë‹ˆì²˜ê°€ ë‹¬ë¼ë„,
    ì‹¤ì œë¡œ ë°›ëŠ” íŒŒë¼ë¯¸í„°ë§Œ ê³¨ë¼ì„œ ì•ˆì „í•˜ê²Œ kwargsë¥¼ êµ¬ì„±í•œë‹¤.
    """
    try:
        sig = inspect.signature(fn)
        allowed = set(sig.parameters.keys())
        return {k: v for k, v in desired.items() if (k in allowed and v is not None)}
    except Exception:
        return {k: v for k, v in desired.items() if v is not None}


def _is_overload_error(e: Exception) -> bool:
    msg = f"{type(e).__name__}: {e}"
    return (
        ("503" in msg)
        or ("UNAVAILABLE" in msg)
        or ("overload" in msg.lower())
        or ("overloaded" in msg.lower())
    )


def _normalize_text_for_windows(text: str) -> str:
    return (text or "").replace("\u00a0", " ").replace("\u200b", " ").strip()


def _has_display() -> bool:
    """
    âœ… Linux ì„œë²„ì—ì„œ headful ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€(ëŒ€ë¶€ë¶„ DISPLAY ì—†ìŒ)
    """
    if not _IS_LINUX:
        return True
    disp = (os.getenv("DISPLAY", "") or "").strip()
    wayland = (os.getenv("WAYLAND_DISPLAY", "") or "").strip()
    return bool(disp or wayland)


def _should_force_headless(headless: bool) -> bool:
    """
    âœ… Linux + DISPLAY ì—†ìŒì´ë©´ headless ê°•ì œ
    """
    if _IS_LINUX and not _has_display():
        return True
    return headless


# ============================================================
# âœ… (ì¶”ê°€) logging -> WebSocket(log_callback) ë¸Œë¦¿ì§€
# ============================================================
class _WSQueueLogHandler(logging.Handler):
    def __init__(self, loop: asyncio.AbstractEventLoop, q: "asyncio.Queue[str]"):
        super().__init__()
        self._loop = loop
        self._q = q

    def emit(self, record: logging.LogRecord) -> None:
        try:
            msg = self.format(record)
            self._loop.call_soon_threadsafe(self._q.put_nowait, msg)
        except Exception:
            pass


class _WSStream:
    def __init__(
        self, loop: asyncio.AbstractEventLoop, q: "asyncio.Queue[str]", prefix: str = ""
    ):
        self._loop = loop
        self._q = q
        self._prefix = prefix
        self._buf = ""

    def write(self, s: str) -> int:
        if not s:
            return 0
        self._buf += s
        while "\n" in self._buf:
            line, self._buf = self._buf.split("\n", 1)
            line = (self._prefix + line).strip()
            if line:
                self._loop.call_soon_threadsafe(self._q.put_nowait, line)
        return len(s)

    def flush(self) -> None:
        line = (self._prefix + (self._buf or "")).strip()
        if line:
            self._loop.call_soon_threadsafe(self._q.put_nowait, line)
        self._buf = ""

    @property
    def encoding(self) -> str:
        return "utf-8"

    @property
    def errors(self) -> str:
        return "replace"


# ============================================================
# âœ… (í•µì‹¬) "íŒŒì¼ ê°ì‹œ" ëŒ€ì‹ , Playwright í˜ì´ì§€ë¥¼ ì§ì ‘ ìº¡ì²˜í•´ì„œ WSë¡œ ìŠ¤íŠ¸ë¦¬ë°
# ============================================================
def _is_page_like(obj: Any) -> bool:
    if obj is None:
        return False
    try:
        shot = getattr(obj, "screenshot", None)
        if callable(shot):
            if hasattr(obj, "goto") or hasattr(obj, "url") or hasattr(obj, "_url"):
                return True
    except Exception:
        return False
    return False


def _deep_find_page_like(root: Any, max_depth: int = 4, max_nodes: int = 400) -> Any:
    if root is None:
        return None

    seen: Set[int] = set()
    q: "deque[Tuple[Any, int]]" = deque([(root, 0)])
    n = 0

    while q and n < max_nodes:
        cur, depth = q.popleft()
        n += 1

        if cur is None:
            continue

        try:
            oid = id(cur)
            if oid in seen:
                continue
            seen.add(oid)
        except Exception:
            pass

        try:
            if _is_page_like(cur):
                return cur
        except Exception:
            pass

        if depth >= max_depth:
            continue

        if isinstance(cur, dict):
            for v in list(cur.values())[:60]:
                if v is not None:
                    q.append((v, depth + 1))
            continue

        if isinstance(cur, (list, tuple, set)):
            for v in list(cur)[:60]:
                if v is not None:
                    q.append((v, depth + 1))
            continue

        try:
            d = getattr(cur, "__dict__", None)
            if isinstance(d, dict):
                for _, v in list(d.items())[:120]:
                    if v is None:
                        continue
                    if isinstance(v, (str, int, float, bool, bytes)):
                        continue
                    q.append((v, depth + 1))
        except Exception:
            pass

        try:
            for name in dir(cur)[:120]:
                if name.startswith("__"):
                    continue
                if name in ("_loop", "_logger"):
                    continue
                try:
                    v = getattr(cur, name, None)
                except Exception:
                    continue
                if v is None:
                    continue
                if inspect.isfunction(v) or inspect.ismodule(v) or inspect.isclass(v):
                    continue
                if isinstance(v, (str, int, float, bool, bytes)):
                    continue
                q.append((v, depth + 1))
        except Exception:
            pass

    return None


def _try_get_playwright_page(browser: Any) -> Any:
    candidates = [
        ("page",),
        ("_page",),
        ("browser_session", "page"),
        ("browser_session", "_page"),
        ("_browser_session", "page"),
        ("_browser_session", "_page"),
        ("session", "page"),
        ("_session", "page"),
        ("manager", "page"),
        ("_manager", "page"),
        ("playwright_page",),
        ("_playwright_page",),
        ("_browser", "contexts"),
        ("_browser",),
        ("_context",),
        ("context",),
    ]

    for path in candidates:
        cur = browser
        ok = True
        for key in path:
            if cur is None:
                ok = False
                break
            cur = getattr(cur, key, None)

        if not ok or cur is None:
            continue

        try:
            if isinstance(cur, list) and cur:
                ctx = cur[-1]
                pages = getattr(ctx, "pages", None)
                if pages:
                    return pages[-1]
            if _is_page_like(cur):
                return cur
        except Exception:
            pass

    ctx = getattr(browser, "context", None) or getattr(browser, "_context", None)
    if ctx is not None:
        try:
            pages = getattr(ctx, "pages", None)
            if pages:
                return pages[-1]
        except Exception:
            pass

    try:
        found = _deep_find_page_like(browser, max_depth=4, max_nodes=400)
        if found is not None:
            return found
    except Exception:
        pass

    return None


async def _pump_live_screenshots(
    browser: Any,
    screenshot_callback: AsyncScreenshotCallback,
    log_callback: AsyncLogCallback = None,
    interval_sec: float = 0.5,
    jpeg_quality: int = 55,
    max_bytes: int = 900_000,
) -> None:
    if not screenshot_callback:
        return

    page = None
    last_warn = 0.0

    while True:
        try:
            if page is None:
                page = _try_get_playwright_page(browser)
                if page is None:
                    now = time.time()
                    if log_callback and (now - last_warn > 2.0):
                        await log_callback(
                            "âš ï¸ live screenshot: Playwright pageë¥¼ ì•„ì§ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤(ì¬ì‹œë„ ì¤‘)..."
                        )
                        last_warn = now
                    await asyncio.sleep(0.2)
                    continue

            buf = await asyncio.wait_for(
                page.screenshot(type="jpeg", quality=jpeg_quality, full_page=False),
                timeout=6.0,
            )
            if buf and len(buf) <= max_bytes:
                b64 = base64.b64encode(buf).decode("utf-8")
                await screenshot_callback(b64)
            else:
                if log_callback and buf:
                    await log_callback(
                        f"âš ï¸ live screenshot too large, skip ({len(buf)} bytes)"
                    )

            await asyncio.sleep(interval_sec)

        except asyncio.CancelledError:
            return
        except asyncio.TimeoutError:
            page = None
            if log_callback:
                try:
                    await log_callback(
                        "âš ï¸ live screenshot timeout(ì¬ì‹œë„): page.screenshot timed out"
                    )
                except Exception:
                    pass
            await asyncio.sleep(0.4)
        except Exception as e:
            page = None
            if log_callback:
                try:
                    await log_callback(f"âš ï¸ live screenshot error(ì¬ì‹œë„): {e}")
                except Exception:
                    pass
            await asyncio.sleep(0.3)


# ============================================================
# âœ… HTML ì´ë¯¸ì§€ URL ì¶”ì¶œ(ê¸°ì¡´ ìœ ì§€)
# ============================================================
_IMG_SRC_RE = re.compile(r"""<img[^>]+src\s*=\s*["']([^"']+)["']""", re.IGNORECASE)
_OG_IMAGE_RE = re.compile(
    r"""<meta[^>]+property\s*=\s*["']og:image["'][^>]+content\s*=\s*["']([^"']+)["']""",
    re.IGNORECASE,
)
_A_HREF_RE = re.compile(r"""<a[^>]+href\s*=\s*["']([^"']+)["']""", re.IGNORECASE)
_IMG_EXT_RE = re.compile(r"""\.(?:png|jpe?g|gif|webp)(?:\?.*)?$""", re.IGNORECASE)


def _dedup_keep_order(items: List[str]) -> List[str]:
    out: List[str] = []
    seen: Set[str] = set()
    for x in items:
        s = (x or "").strip()
        if not s:
            continue
        if s in seen:
            continue
        seen.add(s)
        out.append(s)
    return out


async def _fetch_html(url: str, timeout_sec: float = 10.0) -> str:
    if not url:
        return ""
    url = _normalize_url(url)
    try:
        async with httpx.AsyncClient(
            follow_redirects=True, timeout=timeout_sec
        ) as client:
            r = await client.get(url, headers={"User-Agent": "Mozilla/5.0"})
            r.raise_for_status()
            return r.text or ""
    except Exception as e:
        logger.warning("[BrowserService] HTML fetch failed url=%s err=%s", url, e)
        return ""


def _extract_image_urls_from_html(html: str, base_url: str, limit: int = 30) -> List[str]:
    if not html:
        return []
    found: List[str] = []

    for m in _OG_IMAGE_RE.finditer(html):
        found.append(urljoin(base_url, (m.group(1) or "").strip()))

    for m in _IMG_SRC_RE.finditer(html):
        src = (m.group(1) or "").strip()
        if not src:
            continue
        if src.lower().startswith("data:"):
            continue
        found.append(urljoin(base_url, src))

    for m in _A_HREF_RE.finditer(html):
        href = (m.group(1) or "").strip()
        if not href:
            continue
        if href.lower().startswith("data:"):
            continue
        abs_url = urljoin(base_url, href)
        if _IMG_EXT_RE.search(abs_url):
            found.append(abs_url)

    found = _dedup_keep_order(found)
    return found[: max(0, limit)]


async def _enrich_image_urls_via_related_pages(
    entry_url: str,
    base_image_urls: List[str],
    max_image_urls: int = 30,
    max_related_pages: int = 3,
) -> List[str]:
    if not entry_url:
        return _dedup_keep_order(base_image_urls)[:max_image_urls]

    urls: List[str] = list(base_image_urls or [])

    html = await _fetch_html(entry_url, timeout_sec=10.0)
    urls.extend(_extract_image_urls_from_html(html, entry_url, limit=max_image_urls))

    candidates: List[str] = []
    for m in _A_HREF_RE.finditer(html or ""):
        href = (m.group(1) or "").strip()
        if not href:
            continue
        if href.lower().startswith("javascript:"):
            continue
        abs_url = urljoin(entry_url, href)

        if _IMG_EXT_RE.search(abs_url):
            candidates.append(abs_url)
            continue

        low = abs_url.lower()
        if any(
            k in low
            for k in (
                "amode=view",
                "view",
                "preview",
                "viewer",
                "attach",
                "download",
                "file",
                "atch",
                "origin",
            )
        ):
            candidates.append(abs_url)

    candidates = _dedup_keep_order(candidates)[: max_related_pages]

    for u in candidates:
        if _IMG_EXT_RE.search(u):
            urls.append(u)
            continue
        sub_html = await _fetch_html(u, timeout_sec=10.0)
        urls.extend(_extract_image_urls_from_html(sub_html, u, limit=max_image_urls))

    return _dedup_keep_order(urls)[:max_image_urls]


# ============================================================
# âœ… í›„ì²˜ë¦¬(Validation + Repair) ìœ í‹¸ (ê¸°ì¡´ ìœ ì§€)
# ============================================================
_PHONE_RE = re.compile(r"(?:\+82[-\s]?)?0\d{1,2}[-\s]?\d{3,4}[-\s]?\d{4}")
_LIKELY_ADDRESS_RE = re.compile(
    r"(?:\b(?:ë„ë¡œ|ë¡œ|ê¸¸)\s*\d+\b)|(?:\b\d+\s*ë²ˆì§€\b)|(?:\b(?:ì|ë©´|ë™|ë¦¬)\b)|(?:\b(?:ì‹œ|êµ°|êµ¬)\b)"
)
_FILE_EXT_RE = re.compile(
    r".+\.(?:hwp|hwpx|pdf|docx?|xlsx?|pptx?|zip|png|jpg|jpeg)$", re.IGNORECASE
)
_AGE_HINT_RE = re.compile(
    r"(ë§Œ\s*\d{1,2}\s*ì„¸\s*(?:ì´ìƒ|ì´í•˜))"
    r"|(ë§Œ\s*\d{1,2}\s*ì„¸\s*~\s*ë§Œ\s*\d{1,2}\s*ì„¸)"
    r"|(\d{1,2}\s*ì„¸\s*(?:ì´ìƒ|ì´í•˜))"
)
NONE_VALUES: Set[str] = {"ì—†ìŒ", "ì—†ìŠµë‹ˆë‹¤", "í•´ë‹¹ ì—†ìŒ", "í•´ë‹¹ì—†ìŒ", "ë¬´ê´€", "ë¯¸í•´ë‹¹", "-", "x", "X"}


def _norm_none_or_str(v: Any) -> Optional[str]:
    if v is None:
        return None
    s = str(v).strip()
    return s if s else None


def _normalize_none_value(value: Any, default: str = "ì œí•œ ì—†ìŒ") -> str:
    s = (_norm_none_or_str(value) or "").strip()
    if not s:
        return default
    if s.lower() in ("none", "n/a"):
        return default
    return default if s in NONE_VALUES else s


def _extract_field_line(evidence_text: str, field_name: str) -> Optional[str]:
    if not evidence_text:
        return None
    m = re.search(rf"{re.escape(field_name)}\s*\n\s*([^\n]+)", evidence_text)
    if m:
        return m.group(1).strip()
    m2 = re.search(rf"{re.escape(field_name)}\s*[:\-]?\s*([^\n]+)", evidence_text)
    if m2:
        return m2.group(1).strip()
    return None


def _merge_region_from_evidence(criteria_region: Optional[str], evidence_text: str) -> Optional[str]:
    ev = _extract_field_line(evidence_text, "ì§€ì—­ê¸°ì¤€")
    if not ev:
        return criteria_region
    cur = (criteria_region or "").strip()
    if not cur or _normalize_none_value(cur) == "ì œí•œ ì—†ìŒ":
        return ev
    if ("ìƒê´€ì—†ìŒ" in ev) and ("ìƒê´€ì—†ìŒ" not in cur):
        return ev
    if ("|" in ev) and ("|" not in cur):
        return ev
    return criteria_region


def _clean_required_documents(items: Any) -> List[str]:
    if not isinstance(items, list):
        return []
    out: List[str] = []
    for raw in items:
        s = _norm_none_or_str(raw)
        if not s:
            continue
        if ("ëŒ€í‘œì „í™”" in s) or ("ëŒ€í‘œ ì „í™”" in s) or ("ì „í™”" in s) or ("TEL" in s.upper()):
            if not _FILE_EXT_RE.match(s):
                continue
        if _PHONE_RE.search(s) and not _FILE_EXT_RE.match(s):
            continue
        if _LIKELY_ADDRESS_RE.search(s) and not _FILE_EXT_RE.match(s):
            continue
        if (
            _FILE_EXT_RE.match(s)
            or ("ì‹ ì²­ì„œ" in s)
            or ("ì œì¶œ" in s)
            or ("ì„œë¥˜" in s)
            or ("ì¦ëª…" in s)
            or ("ê³µê³ " in s)
        ):
            out.append(s)

    dedup: List[str] = []
    seen: Set[str] = set()
    for x in out:
        if x not in seen:
            seen.add(x)
            dedup.append(x)
    return dedup


def _infer_age_from_evidence(evidence_text: str) -> Optional[str]:
    if not evidence_text:
        return None
    m = _AGE_HINT_RE.search(evidence_text)
    return m.group(0).strip() if m else None


def _normalize_apply_channel(v: Any) -> Optional[str]:
    s = _norm_none_or_str(v)
    if not s:
        return None
    s2 = s.replace(" ", "")
    has_online = "ì˜¨ë¼ì¸" in s2
    has_visit = "ë°©ë¬¸" in s2
    has_mail = "ìš°í¸" in s2
    kinds = sum([has_online, has_visit, has_mail])
    if kinds >= 2:
        return "í˜¼í•©"
    if has_online:
        return "ì˜¨ë¼ì¸"
    if has_visit:
        return "ë°©ë¬¸"
    if has_mail:
        return "ìš°í¸"
    return s


def _clean_documents(result: Dict[str, Any]) -> Dict[str, Any]:
    result["required_documents"] = _clean_required_documents(result.get("required_documents"))
    return result


def _fix_criteria_mapping(result: Dict[str, Any]) -> Dict[str, Any]:
    criteria = result.get("criteria") or {}
    if not isinstance(criteria, dict):
        criteria = {}
    evidence = _norm_none_or_str(result.get("evidence_text")) or ""

    age = _norm_none_or_str(criteria.get("age"))
    region = _norm_none_or_str(criteria.get("region"))

    if age and (("ê±°ì£¼" in age) or ("ê´€ì™¸" in age) or ("ì§€ì—­" in age) or ("ì£¼ì†Œ" in age)):
        if not region or _normalize_none_value(region) == "ì œí•œ ì—†ìŒ":
            criteria["region"] = age
        criteria["age"] = "ì œí•œ ì—†ìŒ"

    inferred_age = _infer_age_from_evidence(evidence)
    if inferred_age:
        cur_age = _norm_none_or_str(criteria.get("age"))
        if not cur_age or _normalize_none_value(cur_age) == "ì œí•œ ì—†ìŒ":
            criteria["age"] = inferred_age

    merged_region = _merge_region_from_evidence(_norm_none_or_str(criteria.get("region")), evidence)
    if merged_region:
        criteria["region"] = merged_region.strip()

    criteria["age"] = _normalize_none_value(criteria.get("age"))
    criteria["region"] = _normalize_none_value(criteria.get("region"))
    criteria["income"] = _normalize_none_value(criteria.get("income"))
    criteria["employment"] = _normalize_none_value(criteria.get("employment"))
    criteria["other"] = _normalize_none_value(criteria.get("other"), default="ì—†ìŒ")

    result["criteria"] = criteria
    return result


def _normalize_fields(result: Dict[str, Any]) -> Dict[str, Any]:
    result["apply_channel"] = _normalize_apply_channel(result.get("apply_channel"))
    return result


def _enrich_contact(result: Dict[str, Any]) -> Dict[str, Any]:
    evidence = _norm_none_or_str(result.get("evidence_text")) or ""
    contact = result.get("contact") or {}
    if not isinstance(contact, dict):
        contact = {}
    if not _norm_none_or_str(contact.get("tel")):
        m = _PHONE_RE.search(evidence)
        if m:
            contact["tel"] = m.group(0)
    result["contact"] = contact
    return result


def _repair_result(result: Dict[str, Any]) -> Dict[str, Any]:
    result = _clean_documents(result)
    result = _fix_criteria_mapping(result)
    result = _normalize_fields(result)
    result = _enrich_contact(result)
    return result


def _list_files_safe(dirpath: str) -> Set[str]:
    try:
        return set(os.listdir(dirpath))
    except Exception:
        return set()


class BrowserService:
    """
    browser-use + Geminië¥¼ ì‚¬ìš©í•´ì„œ
    ì‹¤ì œ ë¸Œë¼ìš°ì €ë¥¼ ëŒë ¤ ì •ì±… ìê²© ìš”ê±´ì„ ê²€ì¦í•˜ëŠ” ì„œë¹„ìŠ¤.
    """

    @staticmethod
    async def _run_agent(
        task: str,
        entry_url: Optional[str] = None,
        log_callback: AsyncLogCallback = None,
        screenshot_callback: AsyncScreenshotCallback = None,
    ) -> Dict[str, Any]:
        # âœ… ì´ë²¤íŠ¸ë£¨í”„/ì •ì±… í™•ì¸(ë””ë²„ê¹…ìš©)
        try:
            policy_name = type(asyncio.get_event_loop_policy()).__name__
            logger.info("[BrowserService] EVENT LOOP POLICY = %s", policy_name)
            if log_callback:
                await log_callback(f"EVENT LOOP POLICY = {policy_name}")
        except Exception as e:
            logger.info("[BrowserService] event loop policy check failed: %s", e)

        loop = None
        loop_name = "(unknown)"
        try:
            loop = asyncio.get_running_loop()
            loop_name = type(loop).__name__
            logger.info("[BrowserService] RUNNING LOOP = %s", loop_name)
            if log_callback:
                await log_callback(f"RUNNING LOOP = {loop_name}")
        except Exception as e:
            logger.info("[BrowserService] running loop check failed: %s", e)

        # ============================================================
        # âœ… Windows + Selector loopì´ë©´: browser-useë¥¼ ë³„ë„ ìŠ¤ë ˆë“œ(Proactor)ë¡œ ìš°íšŒ
        # ============================================================
        if _IS_WINDOWS and "SelectorEventLoop" in (loop_name or ""):
            main_loop = loop  # type: ignore[assignment]

            async def _call_log(msg: str) -> None:
                if not log_callback:
                    return
                fut = asyncio.run_coroutine_threadsafe(log_callback(msg), main_loop)  # type: ignore[arg-type]
                await asyncio.wrap_future(fut)

            async def _call_shot(b64: str) -> None:
                if not screenshot_callback:
                    return
                fut = asyncio.run_coroutine_threadsafe(screenshot_callback(b64), main_loop)  # type: ignore[arg-type]
                await asyncio.wrap_future(fut)

            if log_callback:
                await log_callback(
                    "âš ï¸ Windows Selector loop ê°ì§€: browser-use ì‹¤í–‰ì„ ë³„ë„ ìŠ¤ë ˆë“œ(Proactor loop)ë¡œ ìš°íšŒí•©ë‹ˆë‹¤."
                )

            async def _agent_core() -> Dict[str, Any]:
                return await BrowserService._run_agent(
                    task=task,
                    entry_url=entry_url,
                    log_callback=_call_log,
                    screenshot_callback=_call_shot,
                )

            def _thread_entry() -> Dict[str, Any]:
                try:
                    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
                except Exception:
                    pass
                return asyncio.run(_agent_core())

            return await asyncio.to_thread(_thread_entry)

        # ============================================================
        # âœ… URL ì •ê·œí™”
        # ============================================================
        if entry_url:
            entry_url = _normalize_url(entry_url)
        if log_callback and entry_url:
            await log_callback(f"ì‹œì‘ URL ì •ê·œí™”: {entry_url}")
        if log_callback:
            await log_callback("ë¸Œë¼ìš°ì € ì„¸ì…˜ ì¤€ë¹„ ì¤‘...")

        os.environ.setdefault("PYTHONIOENCODING", "utf-8")

        downloads_dir = ensure_download_dir()
        logger.info("[BrowserService] Using downloads_dir=%s", downloads_dir)

        # ============================================================
        # âœ… ì„œë²„ ì•ˆì •ì„±: Linux ì„œë²„ì—ì„œ DISPLAY ì—†ìœ¼ë©´ headless ê°•ì œ
        # ============================================================
        headless_env = _env_flag("BROWSER_HEADLESS", "true")
        headless = _should_force_headless(headless_env)

        # keep_open: ì„œë²„ì—ì„  ê¸°ë³¸ falseê°€ ì•ˆì •ì . (envë¡œë§Œ true ê¶Œì¥)
        keep_open_default = "false" if _IS_LINUX else "false"
        keep_open = _env_flag("BROWSER_KEEP_OPEN", keep_open_default)
        if _IS_LINUX and keep_open:
            logger.warning("[BrowserService] BROWSER_KEEP_OPEN=true on Linux server is not recommended (may leak processes).")
            if log_callback:
                await log_callback("âš ï¸ ì„œë²„(Linux)ì—ì„œëŠ” BROWSER_KEEP_OPEN=true ë¹„ì¶”ì²œ(í¬ë¡¬ í”„ë¡œì„¸ìŠ¤ ëˆ„ìˆ˜ ìœ„í—˜). false ê¶Œì¥")
        debug_ui = _env_flag("BROWSER_DEBUG_UI", "false")
        slowmo_ms = _env_int("BROWSER_SLOWMO_MS", 0 if _IS_LINUX else 250)

        collect_images = _env_flag("BROWSER_COLLECT_IMAGE_URLS", "true")
        max_image_urls = _env_int("BROWSER_MAX_IMAGE_URLS", 30)
        follow_related = _env_flag("BROWSER_IMAGE_FOLLOW_RELATED_PAGES", "true")
        max_related_pages = _env_int("BROWSER_IMAGE_MAX_RELATED_PAGES", 3)

        max_snapshot_failures = _env_int("BROWSER_MAX_SNAPSHOT_FAILURES", 2)
        snapshot_failures = 0

        max_actions = _env_int("BROWSER_AGENT_MAX_ACTIONS", 20)
        max_time_sec = float(os.getenv("BROWSER_MAX_TIME_SEC", os.getenv("BROWSER_AGENT_MAX_TIME_SEC", "300")))

        chromium_sandbox = _env_flag("BROWSER_CHROMIUM_SANDBOX", "false")

        allowed_domains_raw = (os.getenv("BROWSER_ALLOWED_DOMAINS", "") or "").strip()
        allowed_domains = [d.strip() for d in allowed_domains_raw.split(",") if d.strip()] or None

        # debug_uiëŠ” ë¡œì»¬ì—ì„œë§Œ ì˜ë¯¸ê°€ ìˆê³ , ì„œë²„ì—ì„  DISPLAY ì—†ì–´ì„œ ê¹¨ì§ â†’ ë¬´ì‹œ(í˜¹ì€ headless ê°•ì œ ìœ ì§€)
        if debug_ui and _has_display():
            headless = False
            keep_open = True
            slowmo_ms = max(slowmo_ms, 150)
        elif debug_ui and not _has_display():
            if log_callback:
                await log_callback("âš ï¸ DEBUG_UIê°€ ì¼œì ¸ìˆì§€ë§Œ ì„œë²„ì— DISPLAYê°€ ì—†ì–´ headlessë¡œ ê°•ì œí•©ë‹ˆë‹¤.")
            headless = True

        max_retries = int(os.getenv("BROWSER_LLM_MAX_RETRIES", "3"))
        base_backoff = float(os.getenv("BROWSER_LLM_BACKOFF_SEC", "2.0"))

        logger.info(
            "[BrowserService] headless=%s keep_open=%s max_retries=%s backoff=%s max_actions=%s max_time=%s allowed_domains=%s",
            headless,
            keep_open,
            max_retries,
            base_backoff,
            max_actions,
            max_time_sec,
            allowed_domains or "(none)",
        )
        if log_callback and _IS_LINUX:
            await log_callback(f"ğŸ§ Linux detected: DISPLAY={'yes' if _has_display() else 'no'} â†’ headless={headless}")

        # ============================================================
        # âœ… Browser ìƒì„± í•¨ìˆ˜ (ì‹¤íŒ¨ ì‹œ fallback/retry ìš©)
        # ============================================================
        async def _create_browser(headless_value: bool) -> Any:
            try:
                default_pw_chrome = "/home/ubuntu/.cache/ms-playwright/chromium-1200/chrome-linux64/chrome"
                browser_exec_path = (os.getenv("BROWSER_EXECUTABLE_PATH", "") or "").strip() or default_pw_chrome

                server_args = [
                    "--no-sandbox",
                    "--disable-dev-shm-usage",
                    "--disable-gpu",
                    "--disable-setuid-sandbox",
                    "--disable-features=TranslateUI",
                    "--disable-background-networking",
                    "--disable-background-timer-throttling",
                    "--disable-renderer-backgrounding",
                    "--disable-breakpad",
                    "--no-first-run",
                    "--no-default-browser-check",
                ]

                desired_kwargs: Dict[str, Any] = {
                    "headless": headless_value,
                    "downloads_path": downloads_dir,
                    "slow_mo": slowmo_ms if slowmo_ms > 0 else None,
                    "executable_path": browser_exec_path,
                    "args": server_args,
                    "chromium_sandbox": chromium_sandbox,
                    "keep_alive": keep_open,
                }

                init_kwargs = _build_kwargs_for_callable(Browser.__init__, desired_kwargs)
                b = Browser(**init_kwargs)

                logger.info("[BrowserService] Using browser executable: %s", browser_exec_path)
                if log_callback:
                    await log_callback(f"âœ… browser executable_path: {browser_exec_path}")
                    await log_callback(f"âœ… headless={headless_value} chromium_sandbox={chromium_sandbox} keep_alive={keep_open}")
                return b
            except Exception as e:
                logger.warning("[BrowserService] Browser init fallback due to: %s", e)
                if log_callback:
                    await log_callback(f"âš ï¸ Browser init fallback: {e}")
                try:
                    return Browser(headless=headless_value, downloads_path=downloads_dir)
                except Exception:
                    return Browser(headless=headless_value)

        browser = await _create_browser(headless)

        llm = ChatGoogle(model="gemini-2.5-pro")

        controller = None
        if Controller is not None:
            try:
                controller = Controller(use_web_search=False)  # type: ignore[call-arg]
            except Exception:
                controller = None

        agent_kwargs: Dict[str, Any] = dict(
            task=_normalize_text_for_windows(task),
            llm=llm,
            browser=browser,
        )
        if controller is not None:
            agent_kwargs["controller"] = controller

        agent_kwargs["max_actions"] = max_actions
        agent_kwargs["max_time"] = max_time_sec
        if allowed_domains:
            agent_kwargs["allowed_domains"] = allowed_domains

        try:
            agent = Agent(**agent_kwargs)
        except TypeError:
            agent = Agent(task=_normalize_text_for_windows(task), llm=llm, browser=browser)

        run_started_at = datetime.now(timezone.utc)
        pre_files = _list_files_safe(downloads_dir)

        if log_callback:
            await log_callback("ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘...")

        # ============================================================
        # âœ… (ì¶”ê°€) Agent/BrowserSession/tools ë¡œê·¸ë¥¼ WSë¡œ ìŠ¤íŠ¸ë¦¬ë°
        # ============================================================
        ws_log_queue: "asyncio.Queue[str]" = asyncio.Queue()
        ws_log_handler: Optional[_WSQueueLogHandler] = None
        ws_log_pump_task: Optional[asyncio.Task] = None
        old_stdout: Optional[Any] = None
        old_stderr: Optional[Any] = None
        attached_logger_names: List[str] = []
        saved_logger_handlers: Dict[str, List[logging.Handler]] = {}
        saved_logger_propagate: Dict[str, bool] = {}

        def _make_console_single_source(logger_names: List[str]) -> None:
            for lname in logger_names:
                lg = logging.getLogger(lname)
                saved_logger_handlers[lname] = list(lg.handlers)
                saved_logger_propagate[lname] = bool(lg.propagate)
                lg.handlers = []
                lg.propagate = True

        async def _pump_ws_logs() -> None:
            if not log_callback:
                return
            while True:
                try:
                    line = await ws_log_queue.get()
                    await log_callback(line)
                except asyncio.CancelledError:
                    return
                except Exception:
                    await asyncio.sleep(0.05)

        if log_callback:
            try:
                loop2 = asyncio.get_running_loop()
                ws_log_handler = _WSQueueLogHandler(loop2, ws_log_queue)
                ws_log_handler.setLevel(logging.INFO)
                ws_log_handler.setFormatter(logging.Formatter("%(message)s"))

                attach_root = _env_flag("BROWSER_WS_ATTACH_ROOT_LOGGER", "true")
                capture_stdio = _env_flag("BROWSER_WS_CAPTURE_STDIO", "false")

                lib_logger_names = [
                    "Agent",
                    "BrowserSession",
                    "tools",
                    "cdp_use.client",
                    "browser_use",
                    "browser_use.agent",
                    "browser_use.browser",
                    "cdp_use",
                    "bubus",
                ]
                if _env_flag("BROWSER_CONSOLE_DEDUP", "true"):
                    _make_console_single_source(lib_logger_names)

                target_logger_names = [""] if attach_root else (lib_logger_names + ["app.services.browser_service"])

                for lname in target_logger_names:
                    lg = logging.getLogger(lname)
                    lg.addHandler(ws_log_handler)
                    lg.propagate = True
                    if lg.level == 0 or lg.level > logging.INFO:
                        lg.setLevel(logging.INFO)
                    attached_logger_names.append(lname)

                ws_log_pump_task = asyncio.create_task(_pump_ws_logs())
                if capture_stdio:
                    try:
                        old_stdout = sys.stdout
                        old_stderr = sys.stderr
                        sys.stdout = _WSStream(loop2, ws_log_queue, prefix="")
                        sys.stderr = _WSStream(loop2, ws_log_queue, prefix="[stderr] ")
                    except Exception as e:
                        await log_callback(f"âš ï¸ stdout/stderr ìº¡ì²˜ ì„¤ì • ì‹¤íŒ¨: {e}")
            except Exception as e:
                await log_callback(f"âš ï¸ WS ë¡œê·¸ ë¸Œë¦¿ì§€ ì„¤ì • ì‹¤íŒ¨: {e}")

        # ============================================================
        # âœ… agent.run() (BrowserStart/CDP ì‹¤íŒ¨ ì‹œ headlessë¡œ 1íšŒ ìë™ ì¬ì‹œë„)
        # ============================================================
        history = None
        last_err: Optional[Exception] = None

        screenshot_task_live: Optional[asyncio.Task] = None
        if screenshot_callback:
            interval_sec = float(os.getenv("BROWSER_LIVE_SHOT_INTERVAL_SEC", "0.5"))
            jpeg_quality = int(os.getenv("BROWSER_LIVE_SHOT_JPEG_QUALITY", "55"))
            max_bytes = int(os.getenv("BROWSER_LIVE_SHOT_MAX_BYTES", "900000"))
            screenshot_task_live = asyncio.create_task(
                _pump_live_screenshots(
                    browser=browser,
                    screenshot_callback=screenshot_callback,
                    log_callback=log_callback,
                    interval_sec=interval_sec,
                    jpeg_quality=jpeg_quality,
                    max_bytes=max_bytes,
                )
            )
            if log_callback:
                await log_callback(
                    f"[live-shot] enabled interval={interval_sec}s quality={jpeg_quality} max_bytes={max_bytes}"
                )

        async def _stop_live_shot() -> None:
            nonlocal screenshot_task_live
            if screenshot_task_live:
                screenshot_task_live.cancel()
                try:
                    await screenshot_task_live
                except Exception:
                    pass
                screenshot_task_live = None

        try:
            for attempt in range(0, max_retries + 1):
                try:
                    if attempt > 0 and log_callback:
                        await log_callback(f"LLM ì˜¤ë²„ë¡œë“œ ì¬ì‹œë„ ì¤‘... (attempt={attempt}/{max_retries})")

                    history = await asyncio.wait_for(agent.run(), timeout=max_time_sec)
                    if history is None:
                        raise RuntimeError("Agent returned None")

                    last_err = None
                    break
                except Exception as e:
                    # âœ… screenshot/DOM watchdog timeout
                    if _is_browser_snapshot_timeout(e):
                        snapshot_failures += 1
                        logger.warning(
                            "[BrowserService] snapshot timeout detected (%s/%s): %s",
                            snapshot_failures,
                            max_snapshot_failures,
                            e,
                        )
                        if snapshot_failures >= max_snapshot_failures:
                            await _stop_live_shot()
                            return {
                                "matched": None,
                                "matched_title": None,
                                "source_url": entry_url,
                                "criteria": {},
                                "required_documents": [],
                                "apply_steps": [],
                                "apply_channel": None,
                                "apply_period": None,
                                "contact": {},
                                "evidence_text": "",
                                "navigation_path": [],
                                "error_message": "Browser snapshot (screenshot/DOM) timeouts occurred repeatedly. Manual review needed.",
                                "downloaded_files": [],
                                "downloads_dir": downloads_dir,
                                "image_urls": [],
                                "confidence": 0.0,
                                "needs_review": True,
                            }

                    # âœ… AWSì—ì„œ í„°ì§„ ì¼€ì´ìŠ¤: BrowserStart/CDP í¬íŠ¸ ì—°ê²° ì‹¤íŒ¨ â†’ headless ì¬ì‹œë„
                    if _IS_LINUX and (not headless) and _is_browser_start_timeout(e):
                        if log_callback:
                            await log_callback("âš ï¸ BrowserStart/CDP ì‹¤íŒ¨ ê°ì§€: ì„œë²„ì—ì„œ headful ì‹¤í–‰ì´ ë¶ˆì•ˆì • â†’ headlessë¡œ 1íšŒ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                        try:
                            await _stop_live_shot()
                            try:
                                await browser.close()  # type: ignore[attr-defined]
                            except Exception:
                                pass

                            headless = True
                            browser = await _create_browser(headless)
                            agent_kwargs["browser"] = browser

                            try:
                                agent = Agent(**agent_kwargs)
                            except TypeError:
                                agent = Agent(task=_normalize_text_for_windows(task), llm=llm, browser=browser)

                            if screenshot_callback:
                                interval_sec = float(os.getenv("BROWSER_LIVE_SHOT_INTERVAL_SEC", "0.5"))
                                jpeg_quality = int(os.getenv("BROWSER_LIVE_SHOT_JPEG_QUALITY", "55"))
                                max_bytes = int(os.getenv("BROWSER_LIVE_SHOT_MAX_BYTES", "900000"))
                                screenshot_task_live = asyncio.create_task(
                                    _pump_live_screenshots(
                                        browser=browser,
                                        screenshot_callback=screenshot_callback,
                                        log_callback=log_callback,
                                        interval_sec=interval_sec,
                                        jpeg_quality=jpeg_quality,
                                        max_bytes=max_bytes,
                                    )
                                )
                            # âœ… ì¬ì‹œë„ëŠ” í•œ ë²ˆë§Œ
                            continue
                        except Exception as e2:
                            if log_callback:
                                await log_callback(f"âŒ headless ì¬ì‹œë„ ì¤€ë¹„ ì¤‘ ì‹¤íŒ¨: {e2}")

                    last_err = e
                    if _is_overload_error(e) and attempt < max_retries:
                        sleep_s = base_backoff * (2 ** attempt)
                        logger.warning("[BrowserService] LLM overload detected. retry in %.1fs: %s", sleep_s, e)
                        await asyncio.sleep(sleep_s)
                        continue
                    raise

            await _stop_live_shot()

            if history is None:
                raise last_err or RuntimeError("Agent run failed with unknown error")

        except asyncio.TimeoutError as e:
            logger.error("[BrowserService] Agent timeout (%.1fs): %s", max_time_sec, e)
            return {
                "matched": None,
                "matched_title": None,
                "source_url": entry_url,
                "criteria": {},
                "required_documents": [],
                "apply_steps": [],
                "apply_channel": None,
                "apply_period": None,
                "contact": {},
                "evidence_text": "",
                "navigation_path": [],
                "navigation_path_warning": "Agent timed out before producing navigation ê¸°ë¡",
                "error_message": f"Agent exceeded {max_time_sec:.0f}s timeout",
                "downloaded_files": [],
                "downloads_dir": downloads_dir,
                "image_urls": [],
                "confidence": 0.0,
                "needs_review": True,
            }
        except Exception as e:
            logger.exception("[BrowserService] Agent failed: %s", e)
            return {
                "matched": None,
                "matched_title": None,
                "source_url": entry_url,
                "criteria": {},
                "required_documents": [],
                "apply_steps": [],
                "apply_channel": None,
                "apply_period": None,
                "contact": {},
                "evidence_text": "",
                "navigation_path": [],
                "navigation_path_warning": "Agent failed before producing navigation ê¸°ë¡",
                "error_message": f"Agent execution failed: {str(e)}",
                "downloaded_files": [],
                "downloads_dir": downloads_dir,
                "image_urls": [],
                "confidence": 0.0,
                "needs_review": True,
            }
        finally:
            # âœ… (ì¶”ê°€) WS ë¡œê·¸ ë¸Œë¦¿ì§€ ì •ë¦¬
            try:
                if old_stdout is not None:
                    try:
                        sys.stdout.flush()
                    except Exception:
                        pass
                    sys.stdout = old_stdout
                if old_stderr is not None:
                    try:
                        sys.stderr.flush()
                    except Exception:
                        pass
                    sys.stderr = old_stderr
            except Exception:
                pass

            if ws_log_pump_task:
                ws_log_pump_task.cancel()
                try:
                    await ws_log_pump_task
                except Exception:
                    pass

            if ws_log_handler:
                for lname in attached_logger_names:
                    try:
                        logging.getLogger(lname).removeHandler(ws_log_handler)
                    except Exception:
                        pass
                try:
                    ws_log_handler.close()
                except Exception:
                    pass

            if saved_logger_handlers:
                for lname, handlers in saved_logger_handlers.items():
                    try:
                        lg = logging.getLogger(lname)
                        lg.handlers = handlers
                        lg.propagate = saved_logger_propagate.get(lname, True)
                    except Exception:
                        pass

            if not keep_open:
                try:
                    await browser.close()  # type: ignore[attr-defined]
                except Exception:
                    pass

        # ============================================================
        # âœ… ë‹¤ìš´ë¡œë“œ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        # ============================================================
        post_files = _list_files_safe(downloads_dir)
        new_files = sorted(list(post_files - pre_files))

        downloaded_files: List[str] = []
        for fn in new_files:
            full = os.path.join(downloads_dir, fn)
            try:
                mtime = datetime.fromtimestamp(os.path.getmtime(full), tz=timezone.utc)
                if mtime >= run_started_at:
                    downloaded_files.append(fn)
                else:
                    downloaded_files.append(fn)
            except Exception:
                downloaded_files.append(fn)

        if log_callback:
            await log_callback("ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ, ê²°ê³¼ íŒŒì‹± ì¤‘...")

        try:
            final_text = history.final_result()  # type: ignore[attr-defined]
        except Exception:
            final_text = str(history)

        final_text = _normalize_text_for_windows(final_text)
        logger.info("[BrowserService] final_result text snippet: %s", final_text[:500])

        parsed = BrowserService._safe_json_loads(final_text)

        if "raw" in parsed and not isinstance(parsed.get("matched"), (bool, str)):
            parsed = {
                "matched": None,
                "matched_title": None,
                "source_url": None,
                "criteria": {},
                "required_documents": [],
                "apply_steps": [],
                "apply_channel": None,
                "apply_period": None,
                "contact": {},
                "evidence_text": final_text,
                "navigation_path": [],
                "error_message": "JSON parsing failed - manual review needed. raw text stored in evidence_text.",
                "image_urls": [],
                "confidence": 0.0,
                "needs_review": True,
            }

        result: Dict[str, Any] = {
            "matched": parsed.get("matched") if "matched" in parsed else None,
            "matched_title": parsed.get("matched_title"),
            "source_url": parsed.get("source_url")
            or parsed.get("final_url")
            or parsed.get("url")
            or entry_url
            or None,
            "criteria": parsed.get("criteria") or {},
            "required_documents": parsed.get("required_documents") or [],
            "apply_steps": parsed.get("apply_steps") or [],
            "apply_channel": parsed.get("apply_channel"),
            "apply_period": parsed.get("apply_period"),
            "contact": parsed.get("contact") or {},
            "evidence_text": parsed.get("evidence_text") or parsed.get("evidence") or final_text,
            "navigation_path": parsed.get("navigation_path") or [],
            "error_message": parsed.get("error_message"),
            "downloaded_files": downloaded_files,
            "downloads_dir": downloads_dir,
            "image_urls": parsed.get("image_urls") or [],
            "confidence": float(parsed.get("confidence") or 0.0),
            "needs_review": bool(parsed.get("needs_review")) if "needs_review" in parsed else False,
        }

        if not isinstance(result["navigation_path"], list):
            result["navigation_path"] = []
        if len(result["navigation_path"]) == 0:
            result["navigation_path_warning"] = "Agent did not record any navigation_path (empty list)"
            if entry_url:
                result["navigation_path"] = [{"action": "open", "label": "start", "url": entry_url, "note": "entry"}]

        result = _repair_result(result)

        try:
            if collect_images:
                cur = result.get("image_urls")
                if not isinstance(cur, list):
                    cur = []
                cur = [str(x).strip() for x in cur if str(x).strip()]

                html_url = _norm_none_or_str(result.get("source_url")) or entry_url or ""
                if html_url:
                    if log_callback:
                        await log_callback("ì´ë¯¸ì§€ URL ìˆ˜ì§‘(URL ë°©ì‹: HTML íŒŒì‹±) ì‹œë„ ì¤‘...")

                    if follow_related:
                        result["image_urls"] = await _enrich_image_urls_via_related_pages(
                            entry_url=_normalize_url(html_url),
                            base_image_urls=cur,
                            max_image_urls=max_image_urls,
                            max_related_pages=max_related_pages,
                        )
                    else:
                        html_url2 = _normalize_url(html_url)
                        html = await _fetch_html(html_url2, timeout_sec=10.0)
                        merged = cur + _extract_image_urls_from_html(html, html_url2, limit=max_image_urls)
                        result["image_urls"] = _dedup_keep_order(merged)[:max_image_urls]
                else:
                    result["image_urls"] = _dedup_keep_order(cur)[:max_image_urls]
        except Exception as e:
            logger.warning("[BrowserService] image url enrichment failed: %s", e)

        if result.get("required_documents") and not result.get("downloaded_files"):
            result["error_message"] = (
                (result.get("error_message") or "")
                + " NOTE: required_documentsì— íŒŒì¼ëª…ì´ ìˆì–´ë„ ì‹¤ì œ ë‹¤ìš´ë¡œë“œëŠ” ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            ).strip()

        return result

    @staticmethod
    def _safe_json_loads(text: str) -> Dict[str, Any]:
        try:
            obj = json.loads(text)
            return obj if isinstance(obj, dict) else {"raw": text}
        except Exception:
            pass

        low = (text or "").lower()
        if any(h in low for h in _NOT_FOUND_HINTS) or any(h.lower() in low for h in _JUDGE_FAIL_HINTS):
            return {
                "matched": False,
                "matched_title": None,
                "source_url": None,
                "criteria": {
                    "age": "ì œí•œ ì—†ìŒ",
                    "region": "ì œí•œ ì—†ìŒ",
                    "income": "ì œí•œ ì—†ìŒ",
                    "employment": "ì œí•œ ì—†ìŒ",
                    "other": "ì—†ìŒ",
                },
                "required_documents": [],
                "apply_steps": [],
                "apply_channel": None,
                "apply_period": None,
                "contact": {},
                "contact": {"org": "", "tel": "", "site": ""},
                "evidence_text": text,
                "navigation_path": [],
                "error_message": "POLICY_NOT_FOUND",
                "image_urls": [],
                "confidence": 0.0,
                "needs_review": False,
            }

        m = re.search(r"```(?:json)?\s*(\{[\s\S]*\})\s*```", text, re.IGNORECASE)
        if m:
            candidate = m.group(1)
            try:
                obj = json.loads(candidate)
                return obj if isinstance(obj, dict) else {"raw": text}
            except Exception:
                pass

        start = text.find("{")
        end = text.rfind("}")
        if start != -1 and end != -1 and end > start:
            candidate = text[start : end + 1]
            try:
                obj = json.loads(candidate)
                return obj if isinstance(obj, dict) else {"raw": text}
            except Exception:
                pass

        return {"raw": text}

    @staticmethod
    async def verify_policy_with_agent(
        policy: Policy,
        log_callback: AsyncLogCallback = None,
        screenshot_callback: AsyncScreenshotCallback = None,
    ) -> Dict[str, Any]:
        title = policy.title or ""
        url = _normalize_url(policy.target_url or "")
        if not url:
            raise ValueError("policy.target_url is empty")
        if log_callback:
            await log_callback(f"ì •ê·œí™”ëœ target_url: {url}")

        anti_loop_rules = """
âœ… ì‹¤íŒ¨/ë£¨í”„ ë°©ì§€ ê·œì¹™(ë§¤ìš° ì¤‘ìš”):
- ê°™ì€ ìœ í˜•ì˜ í–‰ë™(ì˜ˆ: ìŠ¤í¬ë¡¤ë§Œ ë°˜ë³µ, ê°™ì€ ë©”ë‰´ í´ë¦­, ë“œë¡­ë‹¤ìš´ ì„ íƒ)ì„ 2ë²ˆ ì—°ì† ì‹¤íŒ¨í•˜ë©´ ì¦‰ì‹œ ì¤‘ë‹¨í•˜ê³  ë‹¤ë¥¸ ì „ëµìœ¼ë¡œ ì „í™˜í•œë‹¤.
- í˜ì´ì§€ í•˜ë‹¨ì˜ 'Family Site' / ì™¸ë¶€ ì´ë™ìš© ë“œë¡­ë‹¤ìš´(<select>)ì€ ì‚¬ìš©í•˜ì§€ ë§ˆë¼. (timeoutì„ ìœ ë°œí•˜ê³  ì •ì±… ê²€ìƒ‰ì— ë„ì›€ë˜ì§€ ì•ŠìŒ)
- ì •ì±…ì„ ì°¾ê¸° ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ 'ì‚¬ì´íŠ¸ ë‚´ë¶€ ê²€ìƒ‰ì°½/í†µí•©ê²€ìƒ‰/ê³µì§€/ê²Œì‹œíŒ/ê³¼ì •ê²€ìƒ‰' ë“± ë‚´ë¶€ ê¸°ëŠ¥ë§Œ ì‚¬ìš©í•´ë¼.
- 5ë²ˆ ì´ìƒ ìŠ¤í¬ë¡¤í•´ë„ ì •ì±… ë‹¨ì„œ(ì •ì±…ëª…/ê³µê³ ëª…/ìƒì„¸ë§í¬/ê²€ìƒ‰ì°½)ê°€ ì•ˆ ë³´ì´ë©´, ìƒë‹¨ ë©”ë‰´ë¡œ ì´ë™í•´ ë‹¤ë¥¸ ë©”ë‰´(ê³µì§€/ëª¨ì§‘/ê³¼ì •ê²€ìƒ‰ ë“±)ë¥¼ íƒìƒ‰í•œë‹¤.
- 10ë²ˆ ì•¡ì…˜(í´ë¦­/ê²€ìƒ‰/ìŠ¤í¬ë¡¤ í¬í•¨) ì•ˆì— ì •ì±… ë‹¨ì„œë¥¼ ëª» ì°¾ìœ¼ë©´ matched=falseë¡œ ì¢…ë£Œí•œë‹¤.
        """.strip()

        stop_after_extract_rules = """
âœ… ì¢…ë£Œ ê·œì¹™(ë§¤ìš° ì¤‘ìš”):
- 'ì´ìš©ì•ˆë‚´/ì†Œê°œ/ê°€ì´ë“œ' í˜ì´ì§€ë¼ë„, ë„ë¯¼ë¦¬í¬í„°(ì²­ë…„ ë¦¬í¬í„°) ëª¨ì§‘/ìš´ì˜/ì„ ë°œ/ì§€ì› ê´€ë ¨ ì •ë³´ê°€ í™•ì¸ë˜ë©´ ê·¸ í˜ì´ì§€ë¥¼ 'ìµœì„ ì˜ ê³µì‹ ì•ˆë‚´'ë¡œ ê°„ì£¼í•˜ê³  ë” íƒìƒ‰í•˜ì§€ ë§ê³  ì¦‰ì‹œ JSONì„ ì™„ì„±í•´ ì¢…ë£Œí•´ë¼.
- extract(í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ)ë¥¼ í•œ ë²ˆ ì„±ê³µí–ˆê³ , ê·¸ í…ìŠ¤íŠ¸ì— ì•„ë˜ ì¤‘ 2ê°œ ì´ìƒì´ í¬í•¨ë˜ë©´ ì¢…ë£Œí•´ë¼:
  1) ë„ë¯¼ë¦¬í¬í„° ë˜ëŠ” ì²­ë…„ ë¦¬í¬í„°
  2) ëª¨ì§‘/ì„ ë°œ/ìš´ì˜/í™œë™/ì§€ì›/ì‹ ì²­/ì ‘ìˆ˜ ì¤‘ í•˜ë‚˜ ì´ìƒ
  3) ë¬¸ì˜/ë‹´ë‹¹/ì—°ë½ì²˜/ì „í™” ì¤‘ í•˜ë‚˜ ì´ìƒ
- ìœ„ ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ëª»í•˜ë©´ ê³„ì† íƒìƒ‰í•˜ë˜, í›„ë³´ í˜ì´ì§€ëŠ” ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ì—´ì–´ë¼.
        """.strip()

        task = f"""
ë„ˆëŠ” ëŒ€í•œë¯¼êµ­ ì²­ë…„ì •ì±…ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ ì—ì´ì „íŠ¸ì•¼.

ì•„ë˜ ì •ì±…ì˜ ê³µì‹ ì•ˆë‚´ í˜ì´ì§€ì— ì ‘ì†í•´ì„œ,
íŠ¹íˆ 'ì§€ì›ëŒ€ìƒ', 'ì‹ ì²­ìê²©', 'ì„ ì •ê¸°ì¤€', 'ì‹ ì²­ë°©ë²•', 'ì œì¶œì„œë¥˜', 'ì ‘ìˆ˜ì²˜/ë¬¸ì˜'ë¥¼ ì°¾ì•„ì„œ ì •ë¦¬í•´ì•¼ í•œë‹¤.

- ì •ì±… ì œëª©: {title}
- ì ‘ì†í•´ì•¼ í•  URL: {url}

ğŸš« ì ˆëŒ€ ê·œì¹™(ì™¸ë¶€ ê²€ìƒ‰ ê¸ˆì§€):
- DuckDuckGo/Google/Bing/Naver ë“± "ì™¸ë¶€ ê²€ìƒ‰ì—”ì§„" ì‚¬ìš© ê¸ˆì§€.
- ì‚¬ì´íŠ¸ ë‚´ë¶€ì—ì„œë§Œ íƒìƒ‰/ê²€ìƒ‰/ë©”ë‰´ ì´ë™ìœ¼ë¡œ í•´ê²°í•´ë¼.
- ì™¸ë¶€ ê²€ìƒ‰ì´ í•„ìš”í•´ ë³´ì´ë©´, ì‚¬ì´íŠ¸ì˜ "í†µí•©ê²€ìƒ‰/ê²€ìƒ‰/ê³µì§€/ê²Œì‹œíŒ" ê°™ì€ ë‚´ë¶€ ë©”ë‰´ë¥¼ ëŒ€ì‹  ì°¾ì•„ë¼.

âœ… ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™(ì •ì±… ì¼ì¹˜ ê²€ì¦):
- ì§€ê¸ˆ ë³´ê³  ìˆëŠ” í˜ì´ì§€ì˜ ì •ì±…ëª…/ê³µê³ ëª…/ì œëª©ì´ '{title}'ê³¼ ì¼ì¹˜(ë˜ëŠ” ë§¤ìš° ìœ ì‚¬)í•´ì•¼ë§Œ ì¶”ì¶œí•œë‹¤.
- ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ê·¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•˜ì§€ ë§ê³ , ë’¤ë¡œê°€ê¸°/ê²€ìƒ‰/ë‹¤ìŒ ê²°ê³¼ë¡œ ì´ë™í•´ì„œ ë‹¤ì‹œ ì‹œë„í•œë‹¤.
- ìµœëŒ€ 5ê°œì˜ í›„ë³´ í˜ì´ì§€(ê²€ìƒ‰ ê²°ê³¼/ìƒì„¸)ë¥¼ ì—´ì–´ë³´ê³ , ê°€ì¥ ì˜ ë§ëŠ” 1ê°œë¥¼ ì„ íƒí•œë‹¤.
- ê²°êµ­ ë§ëŠ” ì •ì±…ì„ ëª» ì°¾ìœ¼ë©´ matched=falseë¡œ ë°˜í™˜í•œë‹¤.

{anti_loop_rules}

{stop_after_extract_rules}

ì¶”ê°€ ì§€ì‹œ(íŒŒì¼ ë‹¤ìš´ë¡œë“œ/ì´ë¯¸ì§€ URL):
- ê³µê³ ë¬¸/ì²¨ë¶€íŒŒì¼/ì‹ ì²­ì„œ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì´ ìˆìœ¼ë©´ í´ë¦­í•´ì„œ ì‹¤ì œë¡œ íŒŒì¼ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•´ë¼.
- ë‹¤ìš´ë¡œë“œê°€ ë°œìƒí•˜ë©´ ê·¸ íŒŒì¼ëª…(í™•ì¥ì í¬í•¨)ì„ required_documentsì— í¬í•¨í•´ë¼.
- (ì¤‘ìš”) í¬ìŠ¤í„°/ì¹´ë“œë‰´ìŠ¤/ë³¸ë¬¸ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ "ë‹¤ìš´ë¡œë“œí•˜ì§€ ë§ê³ " ì´ë¯¸ì§€ URL(src/og:image/a hrefì˜ jpg/png ë“±)ë§Œ ì°¾ì•„ image_urlsì— ë‹´ì•„ë¼.

ì‘ì—… ë‹¨ê³„:
1. ì§€ì •ëœ URLë¡œ ì´ë™í•œë‹¤.
2. íŒì—…/ì•Œë¦¼ ë“±ì´ ëœ¨ë©´ ëª¨ë‘ ë‹«ëŠ”ë‹¤.
3. ì‚¬ì´íŠ¸ ë‚´ ê²€ìƒ‰ì°½/í†µí•©ê²€ìƒ‰/ì •ì±…ê²€ìƒ‰ì´ ìˆìœ¼ë©´ '{title}'ë¡œ ê²€ìƒ‰í•œë‹¤.
4. í›„ë³´ ê²°ê³¼ë¥¼ ì—´ì–´ë³´ë©° ì •ì±…ëª…ì´ '{title}'ê³¼ ë§ëŠ”ì§€ í™•ì¸í•œë‹¤.
5. ë§ëŠ” ì •ì±…(ê°€ì¥ ìœ ì‚¬í•œ ì •ì±…)ì„ ì°¾ìœ¼ë©´ ì•„ë˜ í•­ëª©ì„ ì¶”ì¶œí•œë‹¤:
    - ì§€ì›ëŒ€ìƒ/ì‹ ì²­ìê²©/ì„ ì •ê¸°ì¤€/ì§€ì›ë‚´ìš©
    - ì‹ ì²­ë°©ë²•/ì‹ ì²­ì ˆì°¨/ì ‘ìˆ˜ë°©ë²•/ì‹ ì²­ê¸°ê°„
    - ì œì¶œì„œë¥˜/í•„ìˆ˜ì„œë¥˜/êµ¬ë¹„ì„œë¥˜
    - ë¬¸ì˜ì²˜/ë‹´ë‹¹ê¸°ê´€/ì „í™”ë²ˆí˜¸/í™ˆí˜ì´ì§€
6. ì„¹ì…˜ì„ ì°¾ê¸° ìœ„í•´ í´ë¦­/íƒ­ ì´ë™ì„ í•˜ë©´, ê·¸ ê²½ë¡œë¥¼ navigation_pathì— ê¸°ë¡í•œë‹¤.
7. ìˆ˜ì§‘í•œ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ìµœì¢… ë‹µë³€ì„ ì¶œë ¥í•œë‹¤.

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ë§Œ ì¶œë ¥í•´ë¼. (ì¶”ê°€ ì„¤ëª…/ë¬¸ì¥ ê¸ˆì§€)

{{
  "source_url": "ì‹¤ì œë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•œ í˜ì´ì§€ URL (ì—†ìœ¼ë©´ null)",
  "matched_title": "í˜„ì¬ í˜ì´ì§€ì—ì„œ í™•ì¸í•œ ì •ì±…ëª…/ê³µê³ ëª…/ì œëª© (ì—†ìœ¼ë©´ null)",
  "matched": true,
  "criteria": {{
    "age": "ì—°ë ¹ ìš”ê±´ì„ í•œêµ­ì–´ë¡œ ê°„ë‹¨ ìš”ì•½. ì—†ìœ¼ë©´ 'ì œí•œ ì—†ìŒ'ì´ë¼ê³  ì ê¸°.",
    "region": "ê±°ì£¼ì§€/ì£¼ì†Œ ìš”ê±´ì´ ìˆìœ¼ë©´ ìš”ì•½, ì—†ìœ¼ë©´ 'ì œí•œ ì—†ìŒ'.",
    "income": "ì†Œë“/ì¬ì‚° ê¸°ì¤€ì´ ìˆìœ¼ë©´ ìš”ì•½, ì—†ìœ¼ë©´ 'ì œí•œ ì—†ìŒ'.",
    "employment": "ì¬ì§/êµ¬ì§/ì°½ì—… ë“± ê³ ìš© ìƒíƒœ ê¸°ì¤€ì´ ìˆìœ¼ë©´ ìš”ì•½, ì—†ìœ¼ë©´ 'ì œí•œ ì—†ìŒ'.",
    "other": "ê¸°íƒ€ ì£¼ìš” ìê²©ìš”ê±´ì„ í•œ ì¤„ë¡œ ì •ë¦¬. ì—†ìœ¼ë©´ 'ì—†ìŒ'."
  }},
  "required_documents": ["ì œì¶œì„œë¥˜ë¥¼ í•­ëª© ë¦¬ìŠ¤íŠ¸ë¡œ", "..."],
  "apply_steps": [
    {{"step": 1, "title": "ë‹¨ê³„ ì œëª©", "detail": "ë¬´ì—‡ì„ í•˜ëŠ”ì§€", "url": "í•´ë‹¹ í˜ì´ì§€ê°€ ìˆìœ¼ë©´"}},
    {{"step": 2, "title": "ë‹¨ê³„ ì œëª©", "detail": "ë¬´ì—‡ì„ í•˜ëŠ”ì§€", "url": null}}
  ],
  "apply_channel": "ì˜¨ë¼ì¸/ë°©ë¬¸/ìš°í¸/í˜¼í•© ì¤‘ í•˜ë‚˜ë¡œ ìš”ì•½",
  "apply_period": "ì‹ ì²­ê¸°ê°„/ìƒì‹œ/ë§ˆê°ì¼ ë“± ì›ë¬¸ ê¸°ë°˜ ìš”ì•½",
  "contact": {{"org": "ê¸°ê´€ëª…", "tel": "ì „í™”", "site": "í™ˆí˜ì´ì§€ URL"}},
  "evidence_text": "ìœ„ ê¸°ì¤€ì„ íŒë‹¨í•˜ëŠ” ê·¼ê±°ê°€ ëœ ì›ë¬¸ ë¬¸ì¥ì„ í•œêµ­ì–´ë¡œ ì—¬ëŸ¬ ì¤„ ì´ì–´ì„œ ë¶™ì—¬ë„£ê¸°.",
  "navigation_path": [
    {{"action":"open","label":"ì‹œì‘ URL","url":"{url}","note":"entry"}},
    {{"action":"click","label":"í´ë¦­í•œ íƒ­/ë²„íŠ¼","url":"ì´ë™í•œ URL","note":"ì™œ í´ë¦­í–ˆëŠ”ì§€"}}
  ],
  "error_message": null,
  "image_urls": ["ì´ë¯¸ì§€ URL(í¬ìŠ¤í„°/ë³¸ë¬¸) ìˆìœ¼ë©´", "..."],
  "confidence": 0.0,
  "needs_review": false
}}
        """.strip()

        return await BrowserService._run_agent(
            task,
            entry_url=url,
            log_callback=log_callback,
            screenshot_callback=screenshot_callback,
        )

    @staticmethod
    async def verify_policy_with_playwright_shortcut(
        policy: Policy,
        navigation_path: List[Dict[str, Any]],
        log_callback: AsyncLogCallback = None,
        screenshot_callback: AsyncScreenshotCallback = None,
    ) -> Dict[str, Any]:
        title = policy.title or ""
        url = _normalize_url(policy.target_url or "")
        if not url:
            raise ValueError("policy.target_url is empty")
        if log_callback:
            await log_callback(f"ì •ê·œí™”ëœ target_url: {url}")

        anti_loop_rules = """
âœ… ì‹¤íŒ¨/ë£¨í”„ ë°©ì§€ ê·œì¹™(ë§¤ìš° ì¤‘ìš”):
- ê°™ì€ ìœ í˜•ì˜ í–‰ë™(ì˜ˆ: ìŠ¤í¬ë¡¤ë§Œ ë°˜ë³µ, ê°™ì€ ë©”ë‰´ í´ë¦­, ë“œë¡­ë‹¤ìš´ ì„ íƒ)ì„ 2ë²ˆ ì—°ì† ì‹¤íŒ¨í•˜ë©´ ì¦‰ì‹œ ì¤‘ë‹¨í•˜ê³  ë‹¤ë¥¸ ì „ëµìœ¼ë¡œ ì „í™˜í•œë‹¤.
- í˜ì´ì§€ í•˜ë‹¨ì˜ 'Family Site' / ì™¸ë¶€ ì´ë™ìš© ë“œë¡­ë‹¤ìš´(<select>)ì€ ì‚¬ìš©í•˜ì§€ ë§ˆë¼. (timeoutì„ ìœ ë°œí•˜ê³  ì •ì±… ê²€ìƒ‰ì— ë„ì›€ë˜ì§€ ì•ŠìŒ)
- ì •ì±…ì„ ì°¾ê¸° ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ 'ì‚¬ì´íŠ¸ ë‚´ë¶€ ê²€ìƒ‰ì°½/í†µí•©ê²€ìƒ‰/ê³µì§€/ê²Œì‹œíŒ/ê³¼ì •ê²€ìƒ‰' ë“± ë‚´ë¶€ ê¸°ëŠ¥ë§Œ ì‚¬ìš©í•´ë¼.
- 5ë²ˆ ì´ìƒ ìŠ¤í¬ë¡¤í•´ë„ ì •ì±… ë‹¨ì„œ(ì •ì±…ëª…/ê³µê³ ëª…/ìƒì„¸ë§í¬/ê²€ìƒ‰ì°½)ê°€ ì•ˆ ë³´ì´ë©´, ìƒë‹¨ ë©”ë‰´ë¡œ ì´ë™í•´ ë‹¤ë¥¸ ë©”ë‰´(ê³µì§€/ëª¨ì§‘/ê³¼ì •ê²€ìƒ‰ ë“±)ë¥¼ íƒìƒ‰í•œë‹¤.
- 10ë²ˆ ì•¡ì…˜(í´ë¦­/ê²€ìƒ‰/ìŠ¤í¬ë¡¤ í¬í•¨) ì•ˆì— ì •ì±… ë‹¨ì„œë¥¼ ëª» ì°¾ìœ¼ë©´ matched=falseë¡œ ì¢…ë£Œí•œë‹¤.
        """.strip()

        stop_after_extract_rules = """
âœ… ì¢…ë£Œ ê·œì¹™(ë§¤ìš° ì¤‘ìš”):
- íŒíŠ¸ ê²½ë¡œë¥¼ ë”°ë¼ê°€ì„œ 'ì´ìš©ì•ˆë‚´/ì†Œê°œ/ê°€ì´ë“œ' ì„±ê²© í˜ì´ì§€ì— ë„ë‹¬í•˜ë”ë¼ë„,
  ë„ë¯¼ë¦¬í¬í„°(ì²­ë…„ ë¦¬í¬í„°) ëª¨ì§‘/ìš´ì˜/ì„ ë°œ/ì‹ ì²­/ë¬¸ì˜ ì •ë³´ê°€ í™•ì¸ë˜ë©´ ê·¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•˜ê³  ì¦‰ì‹œ JSONì„ ì™„ì„±í•´ ì¢…ë£Œí•´ë¼.
- extractë¥¼ í•œ ë²ˆ ì„±ê³µí–ˆê³ , í…ìŠ¤íŠ¸ì— (ë„ë¯¼ë¦¬í¬í„°/ì²­ë…„ë¦¬í¬í„°) + (ì‹ ì²­/ì ‘ìˆ˜/ëª¨ì§‘/ì„ ë°œ/ìš´ì˜/í™œë™) ì¤‘ 1ê°œ ì´ìƒì´ ìˆìœ¼ë©´ ì¢…ë£Œí•´ë¼.
        """.strip()

        path_hint = json.dumps(navigation_path, ensure_ascii=False)

        task = f"""
ë„ˆëŠ” ëŒ€í•œë¯¼êµ­ ì²­ë…„ì •ì±…ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ ì—ì´ì „íŠ¸ì•¼.

ì•„ë˜ ì •ì±…ì˜ ê³µì‹ ì•ˆë‚´ í˜ì´ì§€ì— ì ‘ì†í•´ì„œ,
'ì§€ì›ëŒ€ìƒ', 'ì‹ ì²­ìê²©', 'ì„ ì •ê¸°ì¤€' ì„¹ì…˜ì„ ë¹ ë¥´ê²Œ ì°¾ì•„ì•¼ í•œë‹¤.

- ì •ì±… ì œëª©: {title}
- ì ‘ì†í•´ì•¼ í•  URL: {url}

ì´ì „ ì‹¤í–‰ì—ì„œ ì‚¬ìš©í–ˆë˜ ë„¤ë¹„ê²Œì´ì…˜ ê²½ë¡œ íŒíŠ¸:
{path_hint}

ğŸš« ì ˆëŒ€ ê·œì¹™(ì™¸ë¶€ ê²€ìƒ‰ ê¸ˆì§€):
- ì™¸ë¶€ ê²€ìƒ‰ì—”ì§„ ì‚¬ìš© ê¸ˆì§€. ì‚¬ì´íŠ¸ ë‚´ë¶€ì—ì„œë§Œ í•´ê²°.

âœ… ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™(ì •ì±… ì¼ì¹˜ ê²€ì¦):
- ì§€ê¸ˆ ë³´ê³  ìˆëŠ” ìƒì„¸ í˜ì´ì§€ì˜ ì •ì±…ëª…/ê³µê³ ëª…/ì œëª©ì´ '{title}'ê³¼ ì¼ì¹˜(ë˜ëŠ” ë§¤ìš° ìœ ì‚¬)í•´ì•¼ë§Œ ì¶”ì¶œí•œë‹¤.
- ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ê·¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•˜ì§€ ë§ê³ , ë’¤ë¡œê°€ê¸°/ë‹¤ë¥¸ ê²°ê³¼ë¡œ ì´ë™í•´ì„œ ë‹¤ì‹œ ì‹œë„í•œë‹¤.
- ìµœëŒ€ 5ê°œì˜ í›„ë³´ í˜ì´ì§€ë¥¼ í™•ì¸í•˜ê³ ë„ ëª» ì°¾ìœ¼ë©´ matched=falseë¡œ ë°˜í™˜í•œë‹¤.

{anti_loop_rules}

{stop_after_extract_rules}

ì¶”ê°€ ì§€ì‹œ(íŒŒì¼ ë‹¤ìš´ë¡œë“œ/ì´ë¯¸ì§€ URL):
- ê³µê³ ë¬¸/ì²¨ë¶€íŒŒì¼/ì‹ ì²­ì„œ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì´ ìˆìœ¼ë©´ í´ë¦­í•´ì„œ ì‹¤ì œë¡œ íŒŒì¼ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•´ë¼.
- ë‹¤ìš´ë¡œë“œê°€ ë°œìƒí•˜ë©´ ê·¸ íŒŒì¼ëª…(í™•ì¥ì í¬í•¨)ì„ required_documentsì— í¬í•¨í•´ë¼.
- âœ… (ì¤‘ìš”) í¬ìŠ¤í„°/ì¹´ë“œë‰´ìŠ¤/ë³¸ë¬¸ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ "ë‹¤ìš´ë¡œë“œí•˜ì§€ ë§ê³ " ì´ë¯¸ì§€ URL(src/og:image/a hrefì˜ jpg/png ë“±)ë§Œ ì°¾ì•„ image_urlsì— ë‹´ì•„ë¼.

ì‘ì—… ë‹¨ê³„:
1. URLë¡œ ì´ë™í•œë‹¤.
2. íŒì—…/ì•Œë¦¼ì´ ëœ¨ë©´ ë‹«ëŠ”ë‹¤.
3. íŒíŠ¸ ê²½ë¡œë¥¼ ì°¸ê³ í•´ ìœ ì‚¬í•œ ë²„íŠ¼/íƒ­/ë§í¬ë¥¼ í´ë¦­í•´ë³¸ë‹¤.
4. ì•ˆ ë³´ì´ë©´ '{title}'ì„(ë¥¼) ì‚¬ì´íŠ¸ ë‚´ë¶€ì—ì„œ ê²€ìƒ‰í•˜ê±°ë‚˜ ê´€ë ¨ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•„ íƒìƒ‰í•œë‹¤.
5. ì •ì±…ëª…/ê³µê³ ëª…ì´ '{title}'ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•œë‹¤.
6. ê´€ë ¨ ì„¹ì…˜ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•œ ë’¤ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ë§Œ ì¶œë ¥í•´ë¼. (ì¶”ê°€ ì„¤ëª…/ë¬¸ì¥ ê¸ˆì§€)

{{
  "source_url": "ì‹¤ì œë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•œ í˜ì´ì§€ URL (ì—†ìœ¼ë©´ null)",
  "matched_title": "í˜„ì¬ í˜ì´ì§€ì—ì„œ í™•ì¸í•œ ì •ì±…ëª…/ê³µê³ ëª…/ì œëª© (ì—†ìœ¼ë©´ null)",
  "matched": true,
  "criteria": {{
    "age": "ì—°ë ¹ ìš”ê±´ ìš”ì•½ ë˜ëŠ” 'ì œí•œ ì—†ìŒ'.",
    "region": "ê±°ì£¼ì§€ ìš”ê±´ ìš”ì•½ ë˜ëŠ” 'ì œí•œ ì—†ìŒ'.",
    "income": "ì†Œë“/ì¬ì‚° ê¸°ì¤€ ìš”ì•½ ë˜ëŠ” 'ì œí•œ ì—†ìŒ'.",
    "employment": "ê³ ìš© ìƒíƒœ ê¸°ì¤€ ìš”ì•½ ë˜ëŠ” 'ì œí•œ ì—†ìŒ'.",
    "other": "ê¸°íƒ€ ì£¼ìš” ìê²© ìš”ê±´ í•œ ì¤„ ë˜ëŠ” 'ì—†ìŒ'."
  }},
  "required_documents": ["ì œì¶œì„œë¥˜ë¥¼ í•­ëª© ë¦¬ìŠ¤íŠ¸ë¡œ", "..."],
  "apply_steps": [
    {{"step": 1, "title": "ë‹¨ê³„ ì œëª©", "detail": "ë¬´ì—‡ì„ í•˜ëŠ”ì§€", "url": "í•´ë‹¹ í˜ì´ì§€ê°€ ìˆìœ¼ë©´"}},
    {{"step": 2, "title": "ë‹¨ê³„ ì œëª©", "detail": "ë¬´ì—‡ì„ í•˜ëŠ”ì§€", "url": null}}
  ],
  "apply_channel": "ì˜¨ë¼ì¸/ë°©ë¬¸/ìš°í¸/í˜¼í•© ì¤‘ í•˜ë‚˜ë¡œ ìš”ì•½",
  "apply_period": "ì‹ ì²­ê¸°ê°„/ìƒì‹œ/ë§ˆê°ì¼ ë“± ì›ë¬¸ ê¸°ë°˜ ìš”ì•½",
  "contact": {{"org": "ê¸°ê´€ëª…", "tel": "ì „í™”", "site": "í™ˆí˜ì´ì§€ URL"}},
  "evidence_text": "ê·¼ê±°ê°€ ëœ ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ì—¬ëŸ¬ ì¤„ ì´ì–´ì„œ ë¶™ì—¬ë„£ê¸°.",
  "navigation_path": [
    {{"action":"open","label":"ì‹œì‘ URL","url":"{url}","note":"entry"}},
    {{"action":"click","label":"í´ë¦­í•œ íƒ­/ë²„íŠ¼","url":"ì´ë™í•œ URL","note":"ì™œ í´ë¦­í–ˆëŠ”ì§€"}}
  ],
  "error_message": null,
  "image_urls": ["ì´ë¯¸ì§€ URL(í¬ìŠ¤í„°/ë³¸ë¬¸) ìˆìœ¼ë©´", "..."],
  "confidence": 0.0,
  "needs_review": false
}}
        """.strip()

        if log_callback:
            await log_callback("Playwright ì§€ë¦„ê¸¸(íŒíŠ¸ ê¸°ë°˜) ëª¨ë“œë¡œ ê²€ì¦ ì‹œì‘...")

        return await BrowserService._run_agent(
            task,
            entry_url=url,
            log_callback=log_callback,
            screenshot_callback=screenshot_callback,
        )

    @staticmethod
    def verify_policy_sync(
        policy: Policy,
        navigation_path: Optional[List[Dict[str, Any]]] = None,
        log_callback: Optional[Callable[[str], Any]] = None,
    ) -> Dict[str, Any]:
        async def _runner() -> Dict[str, Any]:
            usable_hint = False
            if navigation_path and isinstance(navigation_path, list):
                if len(navigation_path) >= 2:
                    usable_hint = True
                elif len(navigation_path) == 1:
                    a0 = navigation_path[0] or {}
                    if (a0.get("action") != "open") or (a0.get("note") != "auto-injected"):
                        usable_hint = True

            if usable_hint:
                return await BrowserService.verify_policy_with_playwright_shortcut(
                    policy,
                    navigation_path,
                    None,
                    None,
                )
            return await BrowserService.verify_policy_with_agent(policy, None, None)

        return asyncio.run(_runner())

    @staticmethod
    async def search_policy_pages_async(
        query: str,
        filters: Dict[str, Any] | None = None,
    ) -> List[Dict[str, Any]]:
        logger.info(
            "[BrowserService] search_policy_pages_async called query=%s, filters=%s",
            query,
            filters,
        )
        return []
