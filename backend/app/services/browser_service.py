# app/services/browser_service.py

import asyncio
import json
import logging
import time
import os
import re
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Callable, Awaitable, Set
from urllib.parse import urljoin

import httpx
from dotenv import load_dotenv, find_dotenv

from browser_use import Agent, Browser, ChatGoogle

# âœ… browser-use ë²„ì „ì— ë”°ë¼ Controllerê°€ ì—†ì„ ìˆ˜ ìˆìŒ
try:
    from browser_use import Controller  # type: ignore
except Exception:
    Controller = None  # type: ignore

from app.utils.file_utils import ensure_download_dir
from app.models import Policy

logger = logging.getLogger(__name__)

# âœ… .env ë¡œë”©ì„ "í™•ì‹¤í•˜ê²Œ"
# - ì‹¤í–‰ cwdê°€ backend/app ì—¬ë„ ìƒìœ„ë¡œ ì˜¬ë¼ê°€ë©° .envë¥¼ ì°¾ë„ë¡
# - override ì—¬ë¶€ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´ ê°€ëŠ¥
_DOTENV_PATH = find_dotenv(".env", usecwd=True)
_DOTENV_OVERRIDE = os.getenv("DOTENV_OVERRIDE", "false").lower() in (
    "1",
    "true",
    "yes",
    "y",
    "on",
)
load_dotenv(_DOTENV_PATH, override=_DOTENV_OVERRIDE)
logger.info(
    "[BrowserService] dotenv loaded from: %s (override=%s)",
    _DOTENV_PATH or "(not found)",
    _DOTENV_OVERRIDE,
)

# WebSocket ìª½ì—ì„œëŠ” async ì½œë°±ì„ ì“¸ ìˆ˜ ìˆìœ¼ë‹ˆê¹Œ ì´ë ‡ê²Œ íƒ€ì… ì •ì˜
AsyncLogCallback = Optional[Callable[[str], Awaitable[None]]]

_SCREENSHOT_TIMEOUT_RE = re.compile(r"ScreenshotWatchdog\.on_ScreenshotEvent.*timed out", re.IGNORECASE)
_DOMWATCHDOG_SCREENSHOT_FAIL_RE = re.compile(r"Clean screenshot failed", re.IGNORECASE)

def _is_browser_snapshot_timeout(e: Exception) -> bool:
    msg = f"{type(e).__name__}: {e}"
    return bool(_SCREENSHOT_TIMEOUT_RE.search(msg) or _DOMWATCHDOG_SCREENSHOT_FAIL_RE.search(msg))


def _env_flag(name: str, default: str = "false") -> bool:
    return os.getenv(name, default).lower() in ("1", "true", "yes", "y", "on")


def _env_int(name: str, default: int = 0) -> int:
    try:
        return int(os.getenv(name, str(default)))
    except Exception:
        return default


def _is_overload_error(e: Exception) -> bool:
    """
    Gemini/Provider 503(ì˜¤ë²„ë¡œë“œ) ë¥˜ë¥¼ ë¬¸ìì—´ ê¸°ë°˜ìœ¼ë¡œ ê°ì§€.
    (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ˆì™¸ íƒ€ì…ì´ ë²„ì „ë§ˆë‹¤ ë‹¬ë¼ì„œ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ì²´í¬)
    """
    msg = f"{type(e).__name__}: {e}"
    return (
        ("503" in msg)
        or ("UNAVAILABLE" in msg)
        or ("overload" in msg.lower())
        or ("overloaded" in msg.lower())
    )


def _normalize_text_for_windows(text: str) -> str:
    """
    Windows ì½˜ì†”(cp949)ì—ì„œ í„°ì§€ëŠ” ë¬¸ì(íŠ¹íˆ NBSP \\xa0 ë“±)ë¥¼ ì™„í™”.
    """
    return (text or "").replace("\u00a0", " ").replace("\u200b", " ").strip()


_IMG_SRC_RE = re.compile(r"""<img[^>]+src\s*=\s*["']([^"']+)["']""", re.IGNORECASE)
_OG_IMAGE_RE = re.compile(
    r"""<meta[^>]+property\s*=\s*["']og:image["'][^>]+content\s*=\s*["']([^"']+)["']""",
    re.IGNORECASE,
)
_A_HREF_RE = re.compile(r"""<a[^>]+href\s*=\s*["']([^"']+)["']""", re.IGNORECASE)

# ì´ë¯¸ì§€ URLë¡œ ë³¼ í™•ì¥ì (ì²¨ë¶€/ë°”ë¡œë³´ê¸° ë§í¬ê°€ jpgë¡œ ì§ì ‘ ê±¸ë¦° ì¼€ì´ìŠ¤ ëŒ€ì‘)
_IMG_EXT_RE = re.compile(r"""\.(?:png|jpe?g|gif|webp)(?:\?.*)?$""", re.IGNORECASE)

def _dedup_keep_order(items: List[str]) -> List[str]:
    out: List[str] = []
    seen: Set[str] = set()
    for x in items:
        s = (x or "").strip()
        if not s:
            continue
        if s in seen:
            continue
        seen.add(s)
        out.append(s)
    return out


async def _fetch_html(url: str, timeout_sec: float = 10.0) -> str:
    if not url:
        return ""
    try:
        async with httpx.AsyncClient(
            follow_redirects=True, timeout=timeout_sec
        ) as client:
            r = await client.get(url, headers={"User-Agent": "Mozilla/5.0"})
            r.raise_for_status()
            return r.text or ""
    except Exception as e:
        logger.warning("[BrowserService] HTML fetch failed url=%s err=%s", url, e)
        return ""


def _extract_image_urls_from_html(html: str, base_url: str, limit: int = 30) -> List[str]:
    if not html:
        return []
    found: List[str] = []

    # og:image ìš°ì„ 
    for m in _OG_IMAGE_RE.finditer(html):
        found.append(urljoin(base_url, (m.group(1) or "").strip()))

    # img src
    for m in _IMG_SRC_RE.finditer(html):
        src = (m.group(1) or "").strip()
        if not src:
            continue
        # data:ëŠ” ë„ˆë¬´ í¬ê³  OCR ëŒ€ìƒì¼ ë•Œë„ ì²˜ë¦¬ ë‚œì´ë„ â†‘ â†’ ì œì™¸
        if src.lower().startswith("data:"):
            continue
        found.append(urljoin(base_url, src))
    # âœ… a href ë¡œ ì§ì ‘ ê±¸ë¦° ì´ë¯¸ì§€(ì²¨ë¶€ jpg / ì¹´ë“œë‰´ìŠ¤ íŒŒì¼ ë§í¬)ë„ ìˆ˜ì§‘
    for m in _A_HREF_RE.finditer(html):
        href = (m.group(1) or "").strip()
        if not href:
            continue
        if href.lower().startswith("data:"):
            continue
        abs_url = urljoin(base_url, href)
        if _IMG_EXT_RE.search(abs_url):
            found.append(abs_url)
    found = _dedup_keep_order(found)
    return found[: max(0, limit)]

async def _enrich_image_urls_via_related_pages(
    entry_url: str,
    base_image_urls: List[str],
    max_image_urls: int = 30,
    max_related_pages: int = 3,
) -> List[str]:
    """
    âœ… "ë°”ë¡œë³´ê¸°/ì›ë¬¸ë³´ê¸°/ì²¨ë¶€"ì²˜ëŸ¼ ë³„ë„ ë·°ì–´ í˜ì´ì§€ë¥¼ ì—¬ëŠ” ì‚¬ì´íŠ¸ ëŒ€ì‘:
    - entry_url HTMLì—ì„œ ì´ë¯¸ì§€ URLì„ ìˆ˜ì§‘í•˜ê³ ,
    - ì¶”ê°€ë¡œ 'ê´€ë ¨ ë§í¬' ëª‡ ê°œë¥¼ ê³¨ë¼(fetch) ê·¸ ì•ˆì˜ ì´ë¯¸ì§€ URLë„ ìˆ˜ì§‘
    - ìµœì¢…ì ìœ¼ë¡œ URLë§Œ ë°˜í™˜ (ì´ë¯¸ì§€ íŒŒì¼ ë‹¤ìš´ë¡œë“œëŠ” ì—¬ê¸°ì„œ í•˜ì§€ ì•ŠìŒ)
    """
    if not entry_url:
        return _dedup_keep_order(base_image_urls)[:max_image_urls]

    urls: List[str] = list(base_image_urls or [])

    html = await _fetch_html(entry_url, timeout_sec=10.0)
    urls.extend(_extract_image_urls_from_html(html, entry_url, limit=max_image_urls))

    # ê´€ë ¨ í˜ì´ì§€ í›„ë³´ ë§í¬ ìˆ˜ì§‘ (ê°€ë³ê²Œ: a href ì „ì²´ ì¤‘ "view/ì²¨ë¶€/ì›ë¬¸/ë°”ë¡œë³´ê¸°" ëŠë‚Œ URL)
    candidates: List[str] = []
    for m in _A_HREF_RE.finditer(html or ""):
        href = (m.group(1) or "").strip()
        if not href:
            continue
        if href.lower().startswith("javascript:"):
            continue
        abs_url = urljoin(entry_url, href)

        # ì´ë¯¸ì§€ëŠ” ë°”ë¡œ í›„ë³´ë¡œ
        if _IMG_EXT_RE.search(abs_url):
            candidates.append(abs_url)
            continue

        # "ë°”ë¡œë³´ê¸°/ì›ë¬¸ë³´ê¸°/ì²¨ë¶€/ë‹¤ìš´ë¡œë“œ" ë¥˜ë¡œ ë³´ì´ëŠ” ë§í¬ë§Œ ì†ŒëŸ‰ ì¶”ë ¤ì„œ ì¬-fetch
        low = abs_url.lower()
        if any(k in low for k in ("amode=view", "view", "preview", "viewer", "attach", "download", "file", "atch", "origin")):
            candidates.append(abs_url)

    candidates = _dedup_keep_order(candidates)
    # ë„ˆë¬´ ë§ì´ ëŒë©´ ë¹„ìš©/ì‹œê°„ ì¦ê°€ â†’ ìƒìœ„ Nê°œë§Œ
    candidates = candidates[: max_related_pages]

    for u in candidates:
        # ë§í¬ ìì²´ê°€ ì´ë¯¸ì§€ë©´ ì¶”ê°€
        if _IMG_EXT_RE.search(u):
            urls.append(u)
            continue
        sub_html = await _fetch_html(u, timeout_sec=10.0)
        urls.extend(_extract_image_urls_from_html(sub_html, u, limit=max_image_urls))

    return _dedup_keep_order(urls)[:max_image_urls]

# ============================================================
# âœ… í›„ì²˜ë¦¬(Validation + Repair) ìœ í‹¸
#   - required_documents: ì£¼ì†Œ/ëŒ€í‘œì „í™” ì œê±° + íŒŒì¼/ì„œë¥˜ë§Œ ë‚¨ê¹€
#   - criteria.ageì— ì§€ì—­ì¡°ê±´ ë“¤ì–´ê°€ë©´ regionìœ¼ë¡œ ì´ë™ + ageëŠ” evidenceì—ì„œ ë³´ê°• ì‹œë„
#   - 'ì—†ìŒ'ë¥˜ë¥¼ 'ì œí•œ ì—†ìŒ'ìœ¼ë¡œ í†µì¼
#   - apply_channel ì •ê·œí™”(í˜¼í•©/ì˜¨ë¼ì¸/ë°©ë¬¸/ìš°í¸)
#   - contact.telì´ ë¹„ë©´ evidenceì—ì„œ ë³´ê°•
# ============================================================

_PHONE_RE = re.compile(r"(?:\+82[-\s]?)?0\d{1,2}[-\s]?\d{3,4}[-\s]?\d{4}")

# ì£¼ì†Œë¡œ â€œë„ˆë¬´ ì‰½ê²Œâ€ ì˜¤íŒí•˜ì§€ ì•Šë„ë¡, ëŒ€í‘œì ì¸ íŒ¨í„´ ìœ„ì£¼ë¡œë§Œ
_LIKELY_ADDRESS_RE = re.compile(
    r"(?:\b(?:ë„ë¡œ|ë¡œ|ê¸¸)\s*\d+\b)|(?:\b\d+\s*ë²ˆì§€\b)|(?:\b(?:ì|ë©´|ë™|ë¦¬)\b)|(?:\b(?:ì‹œ|êµ°|êµ¬)\b)"
)

_FILE_EXT_RE = re.compile(
    r".+\.(?:hwp|hwpx|pdf|docx?|xlsx?|pptx?|zip|png|jpg|jpeg)$", re.IGNORECASE
)

_AGE_HINT_RE = re.compile(
    r"(ë§Œ\s*\d{1,2}\s*ì„¸\s*(?:ì´ìƒ|ì´í•˜))"
    r"|(ë§Œ\s*\d{1,2}\s*ì„¸\s*~\s*ë§Œ\s*\d{1,2}\s*ì„¸)"
    r"|(\d{1,2}\s*ì„¸\s*(?:ì´ìƒ|ì´í•˜))"
)

# none/ì—†ìŒ ê³„ì—´ ì •ê·œí™”ìš©
NONE_VALUES: Set[str] = {
    "ìƒê´€ì—†ìŒ",
    "ì—†ìŒ",
    "ì—†ìŠµë‹ˆë‹¤",
    "í•´ë‹¹ ì—†ìŒ",
    "í•´ë‹¹ì—†ìŒ",
    "ë¬´ê´€",
    "ë¯¸í•´ë‹¹",
    "-",
    "x",
    "X",
}


def _norm_none_or_str(v: Any) -> Optional[str]:
    if v is None:
        return None
    s = str(v).strip()
    return s if s else None


def _normalize_none_value(value: Any, default: str = "ì œí•œ ì—†ìŒ") -> str:
    """
    ê³µë°±/None/ì—†ìŒë¥˜ -> default ë¡œ í†µì¼
    """
    s = (_norm_none_or_str(value) or "").strip()
    if not s:
        return default
    if s.lower() in ("none", "n/a"):
        return default
    return default if s in NONE_VALUES else s


def _extract_field_line(evidence_text: str, field_name: str) -> Optional[str]:
    """
    evidence_textì—ì„œ ì˜ˆ:
      ì§€ì—­ê¸°ì¤€
      ì œì£¼ì‹œ ì „ì²´ | ì„œê·€í¬ì‹œ ì „ì²´ | ìƒê´€ì—†ìŒ
    ì²˜ëŸ¼ "í•„ë“œëª…" ë‹¤ìŒ ì¤„ ê°’ì„ ë½‘ì•„ì¤€ë‹¤.
    """
    if not evidence_text:
        return None

    # 1) "í•„ë“œëª…\nê°’" íŒ¨í„´
    m = re.search(rf"{re.escape(field_name)}\s*\n\s*([^\n]+)", evidence_text)
    if m:
        return m.group(1).strip()

    # 2) "í•„ë“œëª… ê°’" ê°™ì€ í•œ ì¤„ íŒ¨í„´(í˜¹ì‹œ ëª¨ë¥¼ ë³€í˜•)
    m2 = re.search(rf"{re.escape(field_name)}\s*[:\-]?\s*([^\n]+)", evidence_text)
    if m2:
        return m2.group(1).strip()
    return None


def _merge_region_from_evidence(criteria_region: Optional[str], evidence_text: str) -> Optional[str]:
    """
    âœ… ì´ë²ˆ ì¼€ì´ìŠ¤ í•´ê²°:
    evidenceì—ëŠ” 'ì œì£¼ì‹œ ì „ì²´ | ì„œê·€í¬ì‹œ ì „ì²´ | ìƒê´€ì—†ìŒ' ì´ ìˆëŠ”ë°
    LLMì´ criteria.regionì—ì„œ 'ìƒê´€ì—†ìŒ'ì„ ëˆ„ë½í•˜ëŠ” ê²½ìš°ê°€ ìˆìŒ.
    -> evidenceì˜ ì§€ì—­ê¸°ì¤€ ë¼ì¸ì„ ìš°ì„  ì‹ ë¢°í•´ ë³´ê°•.
    """
    ev = _extract_field_line(evidence_text, "ì§€ì—­ê¸°ì¤€")
    if not ev:
        return criteria_region

    cur = (criteria_region or "").strip()
    # criteriaê°€ ë¹„ì—ˆìœ¼ë©´ evidenceë¡œ ì±„ì›€
    if not cur or _normalize_none_value(cur) == "ì œí•œ ì—†ìŒ":
        return ev

    # evidenceì— ìƒê´€ì—†ìŒì´ ìˆëŠ”ë° criteriaì— ì—†ìœ¼ë©´ ë³´ê°•
    if ("ìƒê´€ì—†ìŒ" in ev) and ("ìƒê´€ì—†ìŒ" not in cur):
        return ev

    # evidenceê°€ ë” êµ¬ì²´ì (íŒŒì´í”„ êµ¬ë¶„)ì¸ë° criteriaê°€ ì¶•ì•½ë¼ ìˆìœ¼ë©´ evidenceë¡œ
    if ("|" in ev) and ("|" not in cur):
        return ev

    return criteria_region


def _clean_required_documents(items: Any) -> List[str]:
    """
    required_documentsì—ì„œ 'ì£¼ì†Œ/ëŒ€í‘œì „í™”' ê°™ì€ ì¡ìŒì„ ì œê±°í•˜ê³ 
    íŒŒì¼ëª…/ì„œì‹/ì„œë¥˜ë¡œ ë³´ì´ëŠ” ê²ƒë§Œ ìµœëŒ€í•œ ë‚¨ê¸´ë‹¤.
    """
    if not isinstance(items, list):
        return []

    out: List[str] = []
    for raw in items:
        s = _norm_none_or_str(raw)
        if not s:
            continue

        # ì „í™”/ì£¼ì†Œ ì˜¤ì—¼ ì œê±°
        if ("ëŒ€í‘œì „í™”" in s) or ("ëŒ€í‘œ ì „í™”" in s) or ("ì „í™”" in s) or ("TEL" in s.upper()):
            if not _FILE_EXT_RE.match(s):
                continue
        if _PHONE_RE.search(s) and not _FILE_EXT_RE.match(s):
            continue
        if _LIKELY_ADDRESS_RE.search(s) and not _FILE_EXT_RE.match(s):
            continue

        # ì„œë¥˜ì²˜ëŸ¼ ë³´ì´ëŠ” ê²ƒë§Œ í†µê³¼
        if (
            _FILE_EXT_RE.match(s)
            or ("ì‹ ì²­ì„œ" in s)
            or ("ì œì¶œ" in s)
            or ("ì„œë¥˜" in s)
            or ("ì¦ëª…" in s)
            or ("ê³µê³ " in s)
        ):
            out.append(s)

    # ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
    dedup: List[str] = []
    seen: Set[str] = set()
    for x in out:
        if x not in seen:
            seen.add(x)
            dedup.append(x)
    return dedup


def _infer_age_from_evidence(evidence_text: str) -> Optional[str]:
    if not evidence_text:
        return None
    m = _AGE_HINT_RE.search(evidence_text)
    if not m:
        return None
    return m.group(0).strip()


def _normalize_apply_channel(v: Any) -> Optional[str]:
    """
    ìŠ¤í‚¤ë§ˆê°€ 'ì˜¨ë¼ì¸/ë°©ë¬¸/ìš°í¸/í˜¼í•©' ì¤‘ í•˜ë‚˜ë¥¼ ì›í•  ë•Œ, í”í•œ ì¶œë ¥ í”ë“¤ë¦¼ì„ ì •ë¦¬.
    """
    s = _norm_none_or_str(v)
    if not s:
        return None
    s2 = s.replace(" ", "")
    has_online = "ì˜¨ë¼ì¸" in s2
    has_visit = "ë°©ë¬¸" in s2
    has_mail = "ìš°í¸" in s2
    kinds = sum([has_online, has_visit, has_mail])
    if kinds >= 2:
        return "í˜¼í•©"
    if has_online:
        return "ì˜¨ë¼ì¸"
    if has_visit:
        return "ë°©ë¬¸"
    if has_mail:
        return "ìš°í¸"
    return s


# --------- repair ë¶„ë¦¬(ë‹¨ì¼ ì±…ì„) ---------

def _clean_documents(result: Dict[str, Any]) -> Dict[str, Any]:
    result["required_documents"] = _clean_required_documents(result.get("required_documents"))
    return result


def _fix_criteria_mapping(result: Dict[str, Any]) -> Dict[str, Any]:
    criteria = result.get("criteria") or {}
    if not isinstance(criteria, dict):
        criteria = {}

    evidence = _norm_none_or_str(result.get("evidence_text")) or ""

    age = _norm_none_or_str(criteria.get("age"))
    region = _norm_none_or_str(criteria.get("region"))

    # criteria.ageì— ì§€ì—­ ì¡°ê±´ì´ ë“¤ì–´ê°„ í”í•œ ì‹¤ìˆ˜ êµì •
    if age and (("ê±°ì£¼" in age) or ("ê´€ì™¸" in age) or ("ì§€ì—­" in age) or ("ì£¼ì†Œ" in age)):
        if not region or _normalize_none_value(region) == "ì œí•œ ì—†ìŒ":
            criteria["region"] = age
        criteria["age"] = "ì œí•œ ì—†ìŒ"

    # evidenceì—ì„œ ì—°ë ¹ íŒíŠ¸ê°€ ìˆìœ¼ë©´ ë³´ê°•
    inferred_age = _infer_age_from_evidence(evidence)
    if inferred_age:
        cur_age = _norm_none_or_str(criteria.get("age"))
        if not cur_age or _normalize_none_value(cur_age) == "ì œí•œ ì—†ìŒ":
            criteria["age"] = inferred_age

    # regionì€ evidence ê¸°ë°˜ìœ¼ë¡œ í•œ ë²ˆ ë” ë³´ê°•(ìƒê´€ì—†ìŒ ëˆ„ë½ ë“± êµì •)
    merged_region = _merge_region_from_evidence(_norm_none_or_str(criteria.get("region")), evidence)
    if merged_region:
        criteria["region"] = merged_region.strip()

    # none ê°’ í†µì¼
    criteria["age"] = _normalize_none_value(criteria.get("age"))
    criteria["region"] = _normalize_none_value(criteria.get("region"))
    criteria["income"] = _normalize_none_value(criteria.get("income"))
    criteria["employment"] = _normalize_none_value(criteria.get("employment"))
    criteria["other"] = _normalize_none_value(criteria.get("other"), default="ì—†ìŒ")

    result["criteria"] = criteria
    return result


def _normalize_fields(result: Dict[str, Any]) -> Dict[str, Any]:
    result["apply_channel"] = _normalize_apply_channel(result.get("apply_channel"))
    return result


def _enrich_contact(result: Dict[str, Any]) -> Dict[str, Any]:
    evidence = _norm_none_or_str(result.get("evidence_text")) or ""
    contact = result.get("contact") or {}
    if not isinstance(contact, dict):
        contact = {}
    if not _norm_none_or_str(contact.get("tel")):
        m = _PHONE_RE.search(evidence)
        if m:
            contact["tel"] = m.group(0)
    result["contact"] = contact
    return result


def _repair_result(result: Dict[str, Any]) -> Dict[str, Any]:
    """
    LLMì´ ìì£¼ í‹€ë¦¬ëŠ” ë§¤í•‘/ì˜¤ì—¼ì„ ì •ê·œì‹ ê¸°ë°˜ìœ¼ë¡œ êµì •.
    (ë‹¨ì¼ ì±…ì„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬í•˜ì—¬ í˜¸ì¶œ)
    """
    result = _clean_documents(result)
    result = _fix_criteria_mapping(result)
    result = _normalize_fields(result)
    result = _enrich_contact(result)
    return result


def _list_files_safe(dirpath: str) -> Set[str]:
    try:
        return set(os.listdir(dirpath))
    except Exception:
        return set()


class BrowserService:
    """
    browser-use + Geminië¥¼ ì‚¬ìš©í•´ì„œ
    ì‹¤ì œ ë¸Œë¼ìš°ì €ë¥¼ ëŒë ¤ ì •ì±… ìê²© ìš”ê±´ì„ ê²€ì¦í•˜ëŠ” ì„œë¹„ìŠ¤.
    - REST Deep Track: verify_policy_sync() (BackgroundTasks ì—ì„œ í˜¸ì¶œ)
    - WebSocket Deep Track: verify_policy_with_agent / verify_policy_with_playwright_shortcut
    """

    # ======================
    # ê³µí†µ: Agent ì‹¤í–‰ í—¬í¼
    # ======================
    @staticmethod
    async def _run_agent(
        task: str,
        entry_url: Optional[str] = None,
        log_callback: AsyncLogCallback = None,
    ) -> Dict[str, Any]:
        """
        browser-use Agent í•œ ë²ˆ ì‹¤í–‰í•˜ê³ 
        ìµœì¢… ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ íŒŒì‹±í•´ì„œ ë¦¬í„´.
        """
        if log_callback:
            await log_callback("ë¸Œë¼ìš°ì € ì„¸ì…˜ ì¤€ë¹„ ì¤‘...")

        # âœ… Windowsì—ì„œ UTF-8 ê°•ì œ (cp949 ì´ìŠˆ ì™„í™”)
        os.environ.setdefault("PYTHONIOENCODING", "utf-8")

        downloads_dir = ensure_download_dir()
        logger.info("[BrowserService] Using downloads_dir=%s", downloads_dir)

        headless = _env_flag("BROWSER_HEADLESS", "true")
        keep_open = _env_flag("BROWSER_KEEP_OPEN", "false")
        debug_ui = _env_flag("BROWSER_DEBUG_UI", "false")
        # âœ… í´ë¦­/ë“œë¡­ë‹¤ìš´ ì•ˆì •í™”(Windows/ë™ì  DOM ì‚¬ì´íŠ¸ì—ì„œ íŠ¹íˆ ìœ íš¨)
        # í•„ìš”í•˜ë©´ .envì—ì„œ 0ìœ¼ë¡œ ëŒ ìˆ˜ ìˆìŒ
        slowmo_ms = _env_int("BROWSER_SLOWMO_MS", 250)

        collect_images = _env_flag("BROWSER_COLLECT_IMAGE_URLS", "true")
        max_image_urls = _env_int("BROWSER_MAX_IMAGE_URLS", 30)
        # âœ… ë°”ë¡œë³´ê¸°/ì²¨ë¶€ ë·°ì–´ê¹Œì§€ ë”°ë¼ê°€ì„œ URL ìˆ˜ì§‘í• ì§€ (ê¸°ë³¸ true)
        follow_related = _env_flag("BROWSER_IMAGE_FOLLOW_RELATED_PAGES", "true")
        max_related_pages = _env_int("BROWSER_IMAGE_MAX_RELATED_PAGES", 3)

        # âœ… ìŠ¤ëƒ…ìƒ· íƒ€ì„ì•„ì›ƒì´ ë„ˆë¬´ ìì£¼ ë‚˜ë©´ ì¡°ê¸° ì¢…ë£Œ(ë£¨í”„/í´ë¦­ ê¹¨ì§ ë°©ì§€)
        #   - browser-use ìª½ì—ì„œ Clean screenshot timeoutì´ ëˆ„ì ë˜ë©´ stale node í´ë¦­ì´ ì—°ì‡„ë¡œ ë°œìƒí•¨
        #   - ì´ ê°’ì€ ìƒí™© ë”°ë¼ ì¡°ì ˆ ê°€ëŠ¥
        max_snapshot_failures = _env_int("BROWSER_MAX_SNAPSHOT_FAILURES", 2)
        snapshot_failures = 0

        # âœ… runaway ë°©ì§€ ì˜µì…˜
        max_actions = _env_int("BROWSER_AGENT_MAX_ACTIONS", 20)
        # âœ… env ì´ë¦„ í˜¼ì„  ë°©ì§€: BROWSER_MAX_TIME_SEC ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ê°’ fallback
        max_time_sec = float(
            os.getenv(
                "BROWSER_MAX_TIME_SEC",
                os.getenv("BROWSER_AGENT_MAX_TIME_SEC", "300"),
            )
        )

        # âœ… (ì˜µì…˜) í—ˆìš© ë„ë©”ì¸ ì œí•œ (ë²„ì „ì— ë”°ë¼ ë¯¸ì§€ì›ì¼ ìˆ˜ ìˆì–´ tryë¡œ ì²˜ë¦¬)
        allowed_domains_raw = (os.getenv("BROWSER_ALLOWED_DOMAINS", "") or "").strip()
        allowed_domains = [d.strip() for d in allowed_domains_raw.split(",") if d.strip()] or None

        # âœ… debug_uiê°€ ì¼œì ¸ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ì°½ ë³´ì´ê²Œ ê°•ì œ
        if debug_ui:
            headless = False
            keep_open = True

        # âœ… ì˜¤ë²„ë¡œë“œ(503) ìë™ ì¬ì‹œë„ ì˜µì…˜
        max_retries = int(os.getenv("BROWSER_LLM_MAX_RETRIES", "3"))
        base_backoff = float(os.getenv("BROWSER_LLM_BACKOFF_SEC", "2.0"))

        logger.info(
            "[BrowserService] headless=%s keep_open=%s max_retries=%s backoff=%s max_actions=%s max_time=%s allowed_domains=%s",
            headless,
            keep_open,
            max_retries,
            base_backoff,
            max_actions,
            max_time_sec,
            allowed_domains or "(none)",
        )

        # ğŸ”¥ ì¤‘ìš”: downloads_path ë¡œ ì¨ì•¼ í•¨ (download_path ì•„ë‹˜!)
        # âœ… browser-use ë²„ì „ì— ë”°ë¼ slow_mo ì¸ì ìœ ë¬´ê°€ ë‹¬ë¼ì„œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        try:
            browser = Browser(
                headless=headless,
                downloads_path=downloads_dir,
                slow_mo=slowmo_ms if slowmo_ms > 0 else None,
            )
        except TypeError:
            browser = Browser(headless=headless, downloads_path=downloads_dir)

        # Gemini (Google API í‚¤ëŠ” .env ì˜ GOOGLE_API_KEY ì‚¬ìš©)
        llm = ChatGoogle(model="gemini-2.5-pro")

        # âœ… ì™¸ë¶€ ê²€ìƒ‰ ê¸ˆì§€: í”„ë¡¬í”„íŠ¸ë§Œìœ¼ë¡œëŠ” ë¶€ì¡± â†’ Controllerë¡œ ì°¨ë‹¨(ê°€ëŠ¥í•œ ë²„ì „ì—ì„œ)
        controller = None
        if Controller is not None:
            try:
                controller = Controller(use_web_search=False)  # type: ignore[call-arg]
            except Exception:
                controller = None

        # âœ… Agent ìƒì„± (ë²„ì „ë³„ ì¸ì ì°¨ì´ë¥¼ ê³ ë ¤í•´ ë‹¨ê³„ì ìœ¼ë¡œ ì‹œë„)
        agent_kwargs: Dict[str, Any] = dict(
            task=_normalize_text_for_windows(task),
            llm=llm,
            browser=browser,
        )
        if controller is not None:
            agent_kwargs["controller"] = controller

        # ì´ ì˜µì…˜ë“¤ì€ ë²„ì „ì— ë”°ë¼ ë¯¸ì§€ì›ì¼ ìˆ˜ ìˆìŒ -> kwargsë¡œ ë„£ê³  TypeError ì‹œ fallback
        agent_kwargs["max_actions"] = max_actions
        agent_kwargs["max_time"] = max_time_sec
        if allowed_domains:
            agent_kwargs["allowed_domains"] = allowed_domains

        try:
            agent = Agent(**agent_kwargs)
        except TypeError:
            # fallback: ìµœì†Œ ì¸ìë§Œ
            agent = Agent(
                task=_normalize_text_for_windows(task),
                llm=llm,
                browser=browser,
            )

        # âœ… ë‹¤ìš´ë¡œë“œ ìŠ¤ëƒ…ìƒ·: ì‹¤í–‰ ì „/í›„ diffë¡œ â€œì§„ì§œ ë‹¤ìš´ë¡œë“œ ëëŠ”ì§€â€ í™•ì¸
        run_started_at = datetime.now(timezone.utc)
        pre_files = _list_files_safe(downloads_dir)

        if log_callback:
            await log_callback("ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘...")

        try:
            history = None
            last_err: Optional[Exception] = None

            for attempt in range(0, max_retries + 1):
                try:
                    if attempt > 0 and log_callback:
                        await log_callback(
                            f"LLM ì˜¤ë²„ë¡œë“œ ì¬ì‹œë„ ì¤‘... (attempt={attempt}/{max_retries})"
                        )

                    # âœ… max_time_sec ê°•ì œ(Agent ì˜µì…˜ì´ ë¬´ì‹œë  ìˆ˜ ìˆìœ¼ë‹ˆ wait_forë¡œ 2ì¤‘ ì•ˆì „ì¥ì¹˜)
                    history = await asyncio.wait_for(agent.run(), timeout=max_time_sec)
                    if history is None:
                        raise RuntimeError("Agent returned None")

                    last_err = None
                    break
                except Exception as e:
                    # âœ… screenshot/dom snapshot timeout ëˆ„ì  ê°ì§€ â†’ ë” ëŒë©´ í´ë¦­ì´ ê³„ì† ê¹¨ì§
                    #    (browser-use ë‚´ë¶€ watchdog ì´ìŠˆë¡œ ìƒíƒœ ì¸ì‹ì´ ë¬´ë„ˆì§€ê¸° ì‹œì‘í–ˆë‹¤ëŠ” ì‹ í˜¸)
                    if _is_browser_snapshot_timeout(e):
                        snapshot_failures += 1
                        logger.warning(
                            "[BrowserService] snapshot timeout detected (%s/%s): %s",
                            snapshot_failures,
                            max_snapshot_failures,
                            e,
                        )
                        if snapshot_failures >= max_snapshot_failures:
                            return {
                                "matched": None,
                                "matched_title": None,
                                "source_url": entry_url,
                                "criteria": {},
                                "required_documents": [],
                                "apply_steps": [],
                                "apply_channel": None,
                                "apply_period": None,
                                "contact": {},
                                "evidence_text": "",
                                "navigation_path": [],
                                "error_message": "Browser snapshot (screenshot/DOM) timeouts occurred repeatedly. Manual review needed.",
                                "downloaded_files": [],
                                "downloads_dir": downloads_dir,
                                "image_urls": [],
                                "confidence": 0.0,
                                "needs_review": True,
                            }
                            
                    last_err = e
                    if _is_overload_error(e) and attempt < max_retries:
                        sleep_s = base_backoff * (2 ** attempt)
                        logger.warning(
                            "[BrowserService] LLM overload detected. retry in %.1fs: %s",
                            sleep_s,
                            e,
                        )
                        await asyncio.sleep(sleep_s)
                        continue
                    raise

            if history is None:
                raise last_err or RuntimeError("Agent run failed with unknown error")

        except asyncio.TimeoutError as e:
            logger.error("[BrowserService] Agent timeout (%.1fs): %s", max_time_sec, e)
            return {
                "matched": None,
                "matched_title": None,
                "source_url": entry_url,
                "criteria": {},
                "required_documents": [],
                "apply_steps": [],
                "apply_channel": None,
                "apply_period": None,
                "contact": {},
                "evidence_text": "",
                "navigation_path": [],
                "navigation_path_warning": "Agent timed out before producing navigation ê¸°ë¡",
                "error_message": f"Agent exceeded {max_time_sec:.0f}s timeout",
                "downloaded_files": [],
                "downloads_dir": downloads_dir,
                "image_urls": [],
                "confidence": 0.0,
                "needs_review": True,
            }
        except Exception as e:
            logger.exception("[BrowserService] Agent failed: %s", e)
            return {
                "matched": None,
                "matched_title": None,
                "source_url": entry_url,
                "criteria": {},
                "required_documents": [],
                "apply_steps": [],
                "apply_channel": None,
                "apply_period": None,
                "contact": {},
                "evidence_text": "",
                "navigation_path": [],
                "navigation_path_warning": "Agent failed before producing navigation ê¸°ë¡",
                "error_message": f"Agent execution failed: {str(e)}",
                "downloaded_files": [],
                "downloads_dir": downloads_dir,
                "image_urls": [],
                "confidence": 0.0,
                "needs_review": True,
            }
        finally:
            # âœ… GUIë¡œ ë””ë²„ê¹…/ê´€ì°°í•˜ê³  ì‹¶ìœ¼ë©´ ì°½ì„ ë‹«ì§€ ì•Šë„ë¡ ì˜µì…˜ ì œê³µ
            if not keep_open:
                try:
                    await browser.close()  # type: ignore[attr-defined]
                except Exception:
                    pass

        # âœ… ë‹¤ìš´ë¡œë“œ diff ê³„ì‚°
        post_files = _list_files_safe(downloads_dir)
        new_files = sorted(list(post_files - pre_files))

        # mtime ê¸°ë°˜ìœ¼ë¡œë„ í•œë²ˆ ë” ê±°ë¥´ê¸°(ê¸°ì¡´ íŒŒì¼ì´ ì´ë¦„ë§Œ ë°”ë€ŒëŠ” ë“± ë°©ì§€)
        downloaded_files: List[str] = []
        for fn in new_files:
            full = os.path.join(downloads_dir, fn)
            try:
                mtime = datetime.fromtimestamp(os.path.getmtime(full), tz=timezone.utc)
                if mtime >= run_started_at:
                    downloaded_files.append(fn)
                else:
                    downloaded_files.append(fn)  # ì´ë¦„ diffë©´ ì¼ë‹¨ í¬í•¨(ë³´ìˆ˜ì ìœ¼ë¡œ)
            except Exception:
                downloaded_files.append(fn)

        if log_callback:
            await log_callback("ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ, ê²°ê³¼ íŒŒì‹± ì¤‘...")

        # browser-use history ì— final_result()ê°€ ìˆë‹¤ê³  ê°€ì •
        try:
            final_text = history.final_result()  # type: ignore[attr-defined]
        except Exception:
            final_text = str(history)

        final_text = _normalize_text_for_windows(final_text)
        logger.info("[BrowserService] final_result text snippet: %s", final_text[:500])

        parsed = BrowserService._safe_json_loads(final_text)

        # âœ… JSON íŒŒì‹± ì‹¤íŒ¨(=rawë§Œ ìˆìŒ)ë©´ "False"ê°€ ì•„ë‹ˆë¼ "ì•Œ ìˆ˜ ì—†ìŒ(None)"ìœ¼ë¡œ ë¶„ë¦¬
        if "raw" in parsed and not isinstance(parsed.get("matched"), (bool, str)):
            parsed = {
                "matched": None,
                "matched_title": None,
                "source_url": None,
                "criteria": {},
                "required_documents": [],
                "apply_steps": [],
                "apply_channel": None,
                "apply_period": None,
                "contact": {},
                "evidence_text": final_text,
                "navigation_path": [],
                "error_message": "JSON parsing failed - manual review needed. raw text stored in evidence_text.",
                "image_urls": [],
                "confidence": 0.0,
                "needs_review": True,
            }

        result: Dict[str, Any] = {
            # âœ… True/False/None(ì•Œ ìˆ˜ ì—†ìŒ) ê·¸ëŒ€ë¡œ ìœ ì§€
            "matched": parsed.get("matched") if "matched" in parsed else None,
            "matched_title": parsed.get("matched_title"),
            "source_url": parsed.get("source_url")
            or parsed.get("final_url")
            or parsed.get("url")
            or None,
            "criteria": parsed.get("criteria") or {},
            "required_documents": parsed.get("required_documents") or [],
            "apply_steps": parsed.get("apply_steps") or [],
            "apply_channel": parsed.get("apply_channel"),
            "apply_period": parsed.get("apply_period"),
            "contact": parsed.get("contact") or {},
            "evidence_text": parsed.get("evidence_text")
            or parsed.get("evidence")
            or final_text,
            "navigation_path": parsed.get("navigation_path") or [],
            "error_message": parsed.get("error_message"),
            # âœ… ì‹¤ì œ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ëª©ë¡(ì§„ì§œë¡œ downloads_dirì— ìƒê¸´ ê²ƒ)
            "downloaded_files": downloaded_files,
            "downloads_dir": downloads_dir,
            # âœ… í¬ìŠ¤í„°/ë³¸ë¬¸ ì´ë¯¸ì§€ URL(í…ìŠ¤íŠ¸/OCRì€ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ)
            "image_urls": parsed.get("image_urls") or [],
            # âœ… ì‚¬ëŒì´ í™•ì¸í•´ì•¼ í•˜ëŠ”ì§€ í”Œë˜ê·¸(ê¸°ë³¸ê°’)
            "confidence": float(parsed.get("confidence") or 0.0),
            "needs_review": bool(parsed.get("needs_review"))
            if "needs_review" in parsed
            else False,
        }

        # âœ… navigation_path ìë™ ì£¼ì… ì œê±°: â€œí•œ ê²ƒì²˜ëŸ¼ ë³´ì´ëŠ”â€ ë¶€ì‘ìš© ë°©ì§€
        if not isinstance(result["navigation_path"], list):
            result["navigation_path"] = []
        if len(result["navigation_path"]) == 0:
            result["navigation_path_warning"] = (
                "Agent did not record any navigation_path (empty list)"
            )

        # âœ… ì—¬ê¸°ì„œ í›„ì²˜ë¦¬(êµì •) í•œë²ˆ ëŒë¦¬ê³  ë°˜í™˜
        result = _repair_result(result)

        # âœ… image_urls ë³´ê°•: "ì‚¬ì§„ì€ ë¬´ì¡°ê±´ URL ë°©ì‹"
        # - LLM ì¶œë ¥ì´ ìˆë“  ì—†ë“ , ìµœì¢… source_url/entry_url ê¸°ì¤€ìœ¼ë¡œ HTML íŒŒì‹±ì„ í•­ìƒ ìˆ˜í–‰í•´ ë³´ê°•í•œë‹¤.
        try:
            if collect_images:
                cur = result.get("image_urls")
                if not isinstance(cur, list):
                    cur = []
                cur = [str(x).strip() for x in cur if str(x).strip()]

                html_url = _norm_none_or_str(result.get("source_url")) or entry_url or ""
                if html_url:
                    if log_callback:
                        await log_callback("ì´ë¯¸ì§€ URL ìˆ˜ì§‘(URL ë°©ì‹: HTML íŒŒì‹±) ì‹œë„ ì¤‘...")

                    if follow_related:
                        result["image_urls"] = await _enrich_image_urls_via_related_pages(
                            entry_url=html_url,
                            base_image_urls=cur,
                            max_image_urls=max_image_urls,
                            max_related_pages=max_related_pages,
                        )
                    else:
                        html = await _fetch_html(html_url, timeout_sec=10.0)
                        merged = cur + _extract_image_urls_from_html(html, html_url, limit=max_image_urls)
                        result["image_urls"] = _dedup_keep_order(merged)[:max_image_urls]
                else:
                    result["image_urls"] = _dedup_keep_order(cur)[:max_image_urls]
        except Exception as e:
            logger.warning("[BrowserService] image url enrichment failed: %s", e)

        # ë‹¤ìš´ë¡œë“œê°€ ì‹¤ì œë¡œ ì—†ìœ¼ë©´, required_documentsì— íŒŒì¼ëª…ì´ ìˆì–´ë„ â€œë¯¸ë‹¤ìš´ë¡œë“œâ€ì¼ ìˆ˜ ìˆë‹¤ëŠ” íŒíŠ¸
        if result.get("required_documents") and not result.get("downloaded_files"):
            result["error_message"] = (
                (result.get("error_message") or "")
                + " NOTE: required_documentsì— íŒŒì¼ëª…ì´ ìˆì–´ë„ ì‹¤ì œ ë‹¤ìš´ë¡œë“œëŠ” ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            ).strip()

        return result

    @staticmethod
    def _safe_json_loads(text: str) -> Dict[str, Any]:
        """
        LLMì´ JSON ì•ë’¤ì— ì„¤ëª…/ì½”ë“œíœìŠ¤ë¥¼ ë¶™ì—¬ë„ ìµœëŒ€í•œ JSONë§Œ ë½‘ì•„ íŒŒì‹±í•œë‹¤.
        """
        try:
            obj = json.loads(text)
            return obj if isinstance(obj, dict) else {"raw": text}
        except Exception:
            pass

        m = re.search(r"```(?:json)?\s*(\{[\s\S]*\})\s*```", text, re.IGNORECASE)
        if m:
            candidate = m.group(1)
            try:
                obj = json.loads(candidate)
                return obj if isinstance(obj, dict) else {"raw": text}
            except Exception:
                pass

        start = text.find("{")
        end = text.rfind("}")
        if start != -1 and end != -1 and end > start:
            candidate = text[start : end + 1]
            try:
                obj = json.loads(candidate)
                return obj if isinstance(obj, dict) else {"raw": text}
            except Exception:
                pass

        return {"raw": text}

    # ======================
    # 1ì°¨: íƒìƒ‰í˜• Agent ëª¨ë“œ
    # ======================
    @staticmethod
    async def verify_policy_with_agent(
        policy: Policy,
        log_callback: AsyncLogCallback = None,
    ) -> Dict[str, Any]:
        title = policy.title or ""
        url = policy.target_url or ""

        # âœ… ë£¨í”„ ë°©ì§€/ìƒ›ê¸¸ ë°©ì§€ ê·œì¹™(ë“œë¡­ë‹¤ìš´/FamilySiteë¡œ ë¹ ì§€ë©´ timeout ë£¨í”„ê°€ ì¦ìŒ)
        anti_loop_rules = """
âœ… ì‹¤íŒ¨/ë£¨í”„ ë°©ì§€ ê·œì¹™(ë§¤ìš° ì¤‘ìš”):
- ê°™ì€ ìœ í˜•ì˜ í–‰ë™(ì˜ˆ: ìŠ¤í¬ë¡¤ë§Œ ë°˜ë³µ, ê°™ì€ ë©”ë‰´ í´ë¦­, ë“œë¡­ë‹¤ìš´ ì„ íƒ)ì„ 2ë²ˆ ì—°ì† ì‹¤íŒ¨í•˜ë©´ ì¦‰ì‹œ ì¤‘ë‹¨í•˜ê³  ë‹¤ë¥¸ ì „ëµìœ¼ë¡œ ì „í™˜í•œë‹¤.
- í˜ì´ì§€ í•˜ë‹¨ì˜ 'Family Site' / ì™¸ë¶€ ì´ë™ìš© ë“œë¡­ë‹¤ìš´(<select>)ì€ ì‚¬ìš©í•˜ì§€ ë§ˆë¼. (timeoutì„ ìœ ë°œí•˜ê³  ì •ì±… ê²€ìƒ‰ì— ë„ì›€ë˜ì§€ ì•ŠìŒ)
- ì •ì±…ì„ ì°¾ê¸° ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ 'ì‚¬ì´íŠ¸ ë‚´ë¶€ ê²€ìƒ‰ì°½/í†µí•©ê²€ìƒ‰/ê³µì§€/ê²Œì‹œíŒ/ê³¼ì •ê²€ìƒ‰' ë“± ë‚´ë¶€ ê¸°ëŠ¥ë§Œ ì‚¬ìš©í•´ë¼.
- 5ë²ˆ ì´ìƒ ìŠ¤í¬ë¡¤í•´ë„ ì •ì±… ë‹¨ì„œ(ì •ì±…ëª…/ê³µê³ ëª…/ìƒì„¸ë§í¬/ê²€ìƒ‰ì°½)ê°€ ì•ˆ ë³´ì´ë©´, ìƒë‹¨ ë©”ë‰´ë¡œ ì´ë™í•´ ë‹¤ë¥¸ ë©”ë‰´(ê³µì§€/ëª¨ì§‘/ê³¼ì •ê²€ìƒ‰ ë“±)ë¥¼ íƒìƒ‰í•œë‹¤.
- 10ë²ˆ ì•¡ì…˜(í´ë¦­/ê²€ìƒ‰/ìŠ¤í¬ë¡¤ í¬í•¨) ì•ˆì— ì •ì±… ë‹¨ì„œë¥¼ ëª» ì°¾ìœ¼ë©´ matched=falseë¡œ ì¢…ë£Œí•œë‹¤.
        """.strip()

        # âœ… "ë“¤ì–´ê°”ë‹¤ê°€ ë”´ì§“" ë°©ì§€: extractë¡œ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ë¥¼ ì–»ì—ˆìœ¼ë©´ ì¦‰ì‹œ JSON ì¶œë ¥í•˜ê³  ì¢…ë£Œ
        stop_after_extract_rules = """
âœ… ì¢…ë£Œ ê·œì¹™(ë§¤ìš° ì¤‘ìš”):
- 'ì´ìš©ì•ˆë‚´/ì†Œê°œ/ê°€ì´ë“œ' í˜ì´ì§€ë¼ë„, ë„ë¯¼ë¦¬í¬í„°(ì²­ë…„ ë¦¬í¬í„°) ëª¨ì§‘/ìš´ì˜/ì„ ë°œ/ì§€ì› ê´€ë ¨ ì •ë³´ê°€ í™•ì¸ë˜ë©´ ê·¸ í˜ì´ì§€ë¥¼ 'ìµœì„ ì˜ ê³µì‹ ì•ˆë‚´'ë¡œ ê°„ì£¼í•˜ê³  ë” íƒìƒ‰í•˜ì§€ ë§ê³  ì¦‰ì‹œ JSONì„ ì™„ì„±í•´ ì¢…ë£Œí•´ë¼.
- extract(í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ)ë¥¼ í•œ ë²ˆ ì„±ê³µí–ˆê³ , ê·¸ í…ìŠ¤íŠ¸ì— ì•„ë˜ ì¤‘ 2ê°œ ì´ìƒì´ í¬í•¨ë˜ë©´ ì¢…ë£Œí•´ë¼:
  1) ë„ë¯¼ë¦¬í¬í„° ë˜ëŠ” ì²­ë…„ ë¦¬í¬í„°
  2) ëª¨ì§‘/ì„ ë°œ/ìš´ì˜/í™œë™/ì§€ì›/ì‹ ì²­/ì ‘ìˆ˜ ì¤‘ í•˜ë‚˜ ì´ìƒ
  3) ë¬¸ì˜/ë‹´ë‹¹/ì—°ë½ì²˜/ì „í™” ì¤‘ í•˜ë‚˜ ì´ìƒ
- ìœ„ ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ëª»í•˜ë©´ ê³„ì† íƒìƒ‰í•˜ë˜, í›„ë³´ í˜ì´ì§€ëŠ” ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ì—´ì–´ë¼.
        """.strip()

        task = f"""
ë„ˆëŠ” ëŒ€í•œë¯¼êµ­ ì²­ë…„ì •ì±…ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ ì—ì´ì „íŠ¸ì•¼.

ì•„ë˜ ì •ì±…ì˜ ê³µì‹ ì•ˆë‚´ í˜ì´ì§€ì— ì ‘ì†í•´ì„œ,
íŠ¹íˆ 'ì§€ì›ëŒ€ìƒ', 'ì‹ ì²­ìê²©', 'ì„ ì •ê¸°ì¤€', 'ì‹ ì²­ë°©ë²•', 'ì œì¶œì„œë¥˜', 'ì ‘ìˆ˜ì²˜/ë¬¸ì˜'ë¥¼ ì°¾ì•„ì„œ ì •ë¦¬í•´ì•¼ í•œë‹¤.

- ì •ì±… ì œëª©: {title}
- ì ‘ì†í•´ì•¼ í•  URL: {url}

ğŸš« ì ˆëŒ€ ê·œì¹™(ì™¸ë¶€ ê²€ìƒ‰ ê¸ˆì§€):
- DuckDuckGo/Google/Bing/Naver ë“± "ì™¸ë¶€ ê²€ìƒ‰ì—”ì§„" ì‚¬ìš© ê¸ˆì§€.
- ì‚¬ì´íŠ¸ ë‚´ë¶€ì—ì„œë§Œ íƒìƒ‰/ê²€ìƒ‰/ë©”ë‰´ ì´ë™ìœ¼ë¡œ í•´ê²°í•´ë¼.
- ì™¸ë¶€ ê²€ìƒ‰ì´ í•„ìš”í•´ ë³´ì´ë©´, ì‚¬ì´íŠ¸ì˜ "í†µí•©ê²€ìƒ‰/ê²€ìƒ‰/ê³µì§€/ê²Œì‹œíŒ" ê°™ì€ ë‚´ë¶€ ë©”ë‰´ë¥¼ ëŒ€ì‹  ì°¾ì•„ë¼.

âœ… ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™(ì •ì±… ì¼ì¹˜ ê²€ì¦):
- ì§€ê¸ˆ ë³´ê³  ìˆëŠ” í˜ì´ì§€ì˜ ì •ì±…ëª…/ê³µê³ ëª…/ì œëª©ì´ '{title}'ê³¼ ì¼ì¹˜(ë˜ëŠ” ë§¤ìš° ìœ ì‚¬)í•´ì•¼ë§Œ ì¶”ì¶œí•œë‹¤.
- ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ê·¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•˜ì§€ ë§ê³ , ë’¤ë¡œê°€ê¸°/ê²€ìƒ‰/ë‹¤ìŒ ê²°ê³¼ë¡œ ì´ë™í•´ì„œ ë‹¤ì‹œ ì‹œë„í•œë‹¤.
- ìµœëŒ€ 5ê°œì˜ í›„ë³´ í˜ì´ì§€(ê²€ìƒ‰ ê²°ê³¼/ìƒì„¸)ë¥¼ ì—´ì–´ë³´ê³ , ê°€ì¥ ì˜ ë§ëŠ” 1ê°œë¥¼ ì„ íƒí•œë‹¤.
- ê²°êµ­ ë§ëŠ” ì •ì±…ì„ ëª» ì°¾ìœ¼ë©´ matched=falseë¡œ ë°˜í™˜í•œë‹¤.

{anti_loop_rules}

{stop_after_extract_rules}

ì¶”ê°€ ì§€ì‹œ(íŒŒì¼ ë‹¤ìš´ë¡œë“œ/ì´ë¯¸ì§€ URL):
- ê³µê³ ë¬¸/ì²¨ë¶€íŒŒì¼/ì‹ ì²­ì„œ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì´ ìˆìœ¼ë©´ í´ë¦­í•´ì„œ ì‹¤ì œë¡œ íŒŒì¼ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•´ë¼.
- ë‹¤ìš´ë¡œë“œê°€ ë°œìƒí•˜ë©´ ê·¸ íŒŒì¼ëª…(í™•ì¥ì í¬í•¨)ì„ required_documentsì— í¬í•¨í•´ë¼.
- (ì¤‘ìš”) í¬ìŠ¤í„°/ì¹´ë“œë‰´ìŠ¤/ë³¸ë¬¸ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ "ë‹¤ìš´ë¡œë“œí•˜ì§€ ë§ê³ " ì´ë¯¸ì§€ URL(src/og:image/a hrefì˜ jpg/png ë“±)ë§Œ ì°¾ì•„ image_urlsì— ë‹´ì•„ë¼.

ì‘ì—… ë‹¨ê³„:
1. ì§€ì •ëœ URLë¡œ ì´ë™í•œë‹¤.
2. íŒì—…/ì•Œë¦¼ ë“±ì´ ëœ¨ë©´ ëª¨ë‘ ë‹«ëŠ”ë‹¤.
3. ì‚¬ì´íŠ¸ ë‚´ ê²€ìƒ‰ì°½/í†µí•©ê²€ìƒ‰/ì •ì±…ê²€ìƒ‰ì´ ìˆìœ¼ë©´ '{title}'ë¡œ ê²€ìƒ‰í•œë‹¤.
4. í›„ë³´ ê²°ê³¼ë¥¼ ì—´ì–´ë³´ë©° ì •ì±…ëª…ì´ '{title}'ê³¼ ë§ëŠ”ì§€ í™•ì¸í•œë‹¤.
5. ë§ëŠ” ì •ì±…(ê°€ì¥ ìœ ì‚¬í•œ ì •ì±…)ì„ ì°¾ìœ¼ë©´ ì•„ë˜ í•­ëª©ì„ ì¶”ì¶œí•œë‹¤:
    - ì§€ì›ëŒ€ìƒ/ì‹ ì²­ìê²©/ì„ ì •ê¸°ì¤€/ì§€ì›ë‚´ìš©
    - ì‹ ì²­ë°©ë²•/ì‹ ì²­ì ˆì°¨/ì ‘ìˆ˜ë°©ë²•/ì‹ ì²­ê¸°ê°„
    - ì œì¶œì„œë¥˜/í•„ìˆ˜ì„œë¥˜/êµ¬ë¹„ì„œë¥˜
    - ë¬¸ì˜ì²˜/ë‹´ë‹¹ê¸°ê´€/ì „í™”ë²ˆí˜¸/í™ˆí˜ì´ì§€
6. ì„¹ì…˜ì„ ì°¾ê¸° ìœ„í•´ í´ë¦­/íƒ­ ì´ë™ì„ í•˜ë©´, ê·¸ ê²½ë¡œë¥¼ navigation_pathì— ê¸°ë¡í•œë‹¤.
7. ìˆ˜ì§‘í•œ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ìµœì¢… ë‹µë³€ì„ ì¶œë ¥í•œë‹¤.

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ë§Œ ì¶œë ¥í•´ë¼. (ì¶”ê°€ ì„¤ëª…/ë¬¸ì¥ ê¸ˆì§€)

{{
  "source_url": "ì‹¤ì œë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•œ í˜ì´ì§€ URL (ì—†ìœ¼ë©´ null)",
  "matched_title": "í˜„ì¬ í˜ì´ì§€ì—ì„œ í™•ì¸í•œ ì •ì±…ëª…/ê³µê³ ëª…/ì œëª© (ì—†ìœ¼ë©´ null)",
  "matched": true,
  "criteria": {{
    "age": "ì—°ë ¹ ìš”ê±´ì„ í•œêµ­ì–´ë¡œ ê°„ë‹¨ ìš”ì•½. ì—†ìœ¼ë©´ 'ì œí•œ ì—†ìŒ'ì´ë¼ê³  ì ê¸°.",
    "region": "ê±°ì£¼ì§€/ì£¼ì†Œ ìš”ê±´ì´ ìˆìœ¼ë©´ ìš”ì•½, ì—†ìœ¼ë©´ 'ì œí•œ ì—†ìŒ'.",
    "income": "ì†Œë“/ì¬ì‚° ê¸°ì¤€ì´ ìˆìœ¼ë©´ ìš”ì•½, ì—†ìœ¼ë©´ 'ì œí•œ ì—†ìŒ'.",
    "employment": "ì¬ì§/êµ¬ì§/ì°½ì—… ë“± ê³ ìš© ìƒíƒœ ê¸°ì¤€ì´ ìˆìœ¼ë©´ ìš”ì•½, ì—†ìœ¼ë©´ 'ì œí•œ ì—†ìŒ'.",
    "other": "ê¸°íƒ€ ì£¼ìš” ìê²©ìš”ê±´ì„ í•œ ì¤„ë¡œ ì •ë¦¬. ì—†ìœ¼ë©´ 'ì—†ìŒ'."
  }},
  "required_documents": ["ì œì¶œì„œë¥˜ë¥¼ í•­ëª© ë¦¬ìŠ¤íŠ¸ë¡œ", "..."],
  "apply_steps": [
    {{"step": 1, "title": "ë‹¨ê³„ ì œëª©", "detail": "ë¬´ì—‡ì„ í•˜ëŠ”ì§€", "url": "í•´ë‹¹ í˜ì´ì§€ê°€ ìˆìœ¼ë©´"}},
    {{"step": 2, "title": "ë‹¨ê³„ ì œëª©", "detail": "ë¬´ì—‡ì„ í•˜ëŠ”ì§€", "url": null}}
  ],
  "apply_channel": "ì˜¨ë¼ì¸/ë°©ë¬¸/ìš°í¸/í˜¼í•© ì¤‘ í•˜ë‚˜ë¡œ ìš”ì•½",
  "apply_period": "ì‹ ì²­ê¸°ê°„/ìƒì‹œ/ë§ˆê°ì¼ ë“± ì›ë¬¸ ê¸°ë°˜ ìš”ì•½",
  "contact": {{"org": "ê¸°ê´€ëª…", "tel": "ì „í™”", "site": "í™ˆí˜ì´ì§€ URL"}},
  "evidence_text": "ìœ„ ê¸°ì¤€ì„ íŒë‹¨í•˜ëŠ” ê·¼ê±°ê°€ ëœ ì›ë¬¸ ë¬¸ì¥ì„ í•œêµ­ì–´ë¡œ ì—¬ëŸ¬ ì¤„ ì´ì–´ì„œ ë¶™ì—¬ë„£ê¸°.",
  "navigation_path": [
    {{"action":"open","label":"ì‹œì‘ URL","url":"{url}","note":"entry"}},
    {{"action":"click","label":"í´ë¦­í•œ íƒ­/ë²„íŠ¼","url":"ì´ë™í•œ URL","note":"ì™œ í´ë¦­í–ˆëŠ”ì§€"}}
  ],
  "error_message": null,
  "image_urls": ["ì´ë¯¸ì§€ URL(í¬ìŠ¤í„°/ë³¸ë¬¸) ìˆìœ¼ë©´", "..."],
  "confidence": 0.0,
  "needs_review": false
}}
        """.strip()

        return await BrowserService._run_agent(task, entry_url=url, log_callback=log_callback)

    # =================================
    # 2ì°¨: navigation_path ì¬ì‚¬ìš© ëª¨ë“œ
    # =================================
    @staticmethod
    async def verify_policy_with_playwright_shortcut(
        policy: Policy,
        navigation_path: List[Dict[str, Any]],
        log_callback: AsyncLogCallback = None,
    ) -> Dict[str, Any]:
        title = policy.title or ""
        url = policy.target_url or ""

        # âœ… ë£¨í”„ ë°©ì§€/ìƒ›ê¸¸ ë°©ì§€ ê·œì¹™(ë“œë¡­ë‹¤ìš´/FamilySiteë¡œ ë¹ ì§€ë©´ timeout ë£¨í”„ê°€ ì¦ìŒ)
        anti_loop_rules = """
âœ… ì‹¤íŒ¨/ë£¨í”„ ë°©ì§€ ê·œì¹™(ë§¤ìš° ì¤‘ìš”):
- ê°™ì€ ìœ í˜•ì˜ í–‰ë™(ì˜ˆ: ìŠ¤í¬ë¡¤ë§Œ ë°˜ë³µ, ê°™ì€ ë©”ë‰´ í´ë¦­, ë“œë¡­ë‹¤ìš´ ì„ íƒ)ì„ 2ë²ˆ ì—°ì† ì‹¤íŒ¨í•˜ë©´ ì¦‰ì‹œ ì¤‘ë‹¨í•˜ê³  ë‹¤ë¥¸ ì „ëµìœ¼ë¡œ ì „í™˜í•œë‹¤.
- í˜ì´ì§€ í•˜ë‹¨ì˜ 'Family Site' / ì™¸ë¶€ ì´ë™ìš© ë“œë¡­ë‹¤ìš´(<select>)ì€ ì‚¬ìš©í•˜ì§€ ë§ˆë¼. (timeoutì„ ìœ ë°œí•˜ê³  ì •ì±… ê²€ìƒ‰ì— ë„ì›€ë˜ì§€ ì•ŠìŒ)
- ì •ì±…ì„ ì°¾ê¸° ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ 'ì‚¬ì´íŠ¸ ë‚´ë¶€ ê²€ìƒ‰ì°½/í†µí•©ê²€ìƒ‰/ê³µì§€/ê²Œì‹œíŒ/ê³¼ì •ê²€ìƒ‰' ë“± ë‚´ë¶€ ê¸°ëŠ¥ë§Œ ì‚¬ìš©í•´ë¼.
- 5ë²ˆ ì´ìƒ ìŠ¤í¬ë¡¤í•´ë„ ì •ì±… ë‹¨ì„œ(ì •ì±…ëª…/ê³µê³ ëª…/ìƒì„¸ë§í¬/ê²€ìƒ‰ì°½)ê°€ ì•ˆ ë³´ì´ë©´, ìƒë‹¨ ë©”ë‰´ë¡œ ì´ë™í•´ ë‹¤ë¥¸ ë©”ë‰´(ê³µì§€/ëª¨ì§‘/ê³¼ì •ê²€ìƒ‰ ë“±)ë¥¼ íƒìƒ‰í•œë‹¤.
- 10ë²ˆ ì•¡ì…˜(í´ë¦­/ê²€ìƒ‰/ìŠ¤í¬ë¡¤ í¬í•¨) ì•ˆì— ì •ì±… ë‹¨ì„œë¥¼ ëª» ì°¾ìœ¼ë©´ matched=falseë¡œ ì¢…ë£Œí•œë‹¤.
        """.strip()

        stop_after_extract_rules = """
âœ… ì¢…ë£Œ ê·œì¹™(ë§¤ìš° ì¤‘ìš”):
- íŒíŠ¸ ê²½ë¡œë¥¼ ë”°ë¼ê°€ì„œ 'ì´ìš©ì•ˆë‚´/ì†Œê°œ/ê°€ì´ë“œ' ì„±ê²© í˜ì´ì§€ì— ë„ë‹¬í•˜ë”ë¼ë„,
  ë„ë¯¼ë¦¬í¬í„°(ì²­ë…„ ë¦¬í¬í„°) ëª¨ì§‘/ìš´ì˜/ì„ ë°œ/ì‹ ì²­/ë¬¸ì˜ ì •ë³´ê°€ í™•ì¸ë˜ë©´ ê·¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•˜ê³  ì¦‰ì‹œ JSONì„ ì™„ì„±í•´ ì¢…ë£Œí•´ë¼.
- extractë¥¼ í•œ ë²ˆ ì„±ê³µí–ˆê³ , í…ìŠ¤íŠ¸ì— (ë„ë¯¼ë¦¬í¬í„°/ì²­ë…„ë¦¬í¬í„°) + (ì‹ ì²­/ì ‘ìˆ˜/ëª¨ì§‘/ì„ ë°œ/ìš´ì˜/í™œë™) ì¤‘ 1ê°œ ì´ìƒì´ ìˆìœ¼ë©´ ì¢…ë£Œí•´ë¼.
        """.strip()

        path_hint = json.dumps(navigation_path, ensure_ascii=False)

        task = f"""
ë„ˆëŠ” ëŒ€í•œë¯¼êµ­ ì²­ë…„ì •ì±…ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ ì—ì´ì „íŠ¸ì•¼.

ì•„ë˜ ì •ì±…ì˜ ê³µì‹ ì•ˆë‚´ í˜ì´ì§€ì— ì ‘ì†í•´ì„œ,
'ì§€ì›ëŒ€ìƒ', 'ì‹ ì²­ìê²©', 'ì„ ì •ê¸°ì¤€' ì„¹ì…˜ì„ ë¹ ë¥´ê²Œ ì°¾ì•„ì•¼ í•œë‹¤.

- ì •ì±… ì œëª©: {title}
- ì ‘ì†í•´ì•¼ í•  URL: {url}

ì´ì „ ì‹¤í–‰ì—ì„œ ì‚¬ìš©í–ˆë˜ ë„¤ë¹„ê²Œì´ì…˜ ê²½ë¡œ íŒíŠ¸:
{path_hint}

ğŸš« ì ˆëŒ€ ê·œì¹™(ì™¸ë¶€ ê²€ìƒ‰ ê¸ˆì§€):
- ì™¸ë¶€ ê²€ìƒ‰ì—”ì§„ ì‚¬ìš© ê¸ˆì§€. ì‚¬ì´íŠ¸ ë‚´ë¶€ì—ì„œë§Œ í•´ê²°.

âœ… ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™(ì •ì±… ì¼ì¹˜ ê²€ì¦):
- ì§€ê¸ˆ ë³´ê³  ìˆëŠ” ìƒì„¸ í˜ì´ì§€ì˜ ì •ì±…ëª…/ê³µê³ ëª…/ì œëª©ì´ '{title}'ê³¼ ì¼ì¹˜(ë˜ëŠ” ë§¤ìš° ìœ ì‚¬)í•´ì•¼ë§Œ ì¶”ì¶œí•œë‹¤.
- ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ê·¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•˜ì§€ ë§ê³ , ë’¤ë¡œê°€ê¸°/ë‹¤ë¥¸ ê²°ê³¼ë¡œ ì´ë™í•´ì„œ ë‹¤ì‹œ ì‹œë„í•œë‹¤.
- ìµœëŒ€ 5ê°œì˜ í›„ë³´ í˜ì´ì§€ë¥¼ í™•ì¸í•˜ê³ ë„ ëª» ì°¾ìœ¼ë©´ matched=falseë¡œ ë°˜í™˜í•œë‹¤.

{anti_loop_rules}

{stop_after_extract_rules}

ì¶”ê°€ ì§€ì‹œ(íŒŒì¼ ë‹¤ìš´ë¡œë“œ/ì´ë¯¸ì§€ URL):
- ê³µê³ ë¬¸/ì²¨ë¶€íŒŒì¼/ì‹ ì²­ì„œ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì´ ìˆìœ¼ë©´ í´ë¦­í•´ì„œ ì‹¤ì œë¡œ íŒŒì¼ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•´ë¼.
- ë‹¤ìš´ë¡œë“œê°€ ë°œìƒí•˜ë©´ ê·¸ íŒŒì¼ëª…(í™•ì¥ì í¬í•¨)ì„ required_documentsì— í¬í•¨í•´ë¼.
- âœ… (ì¤‘ìš”) í¬ìŠ¤í„°/ì¹´ë“œë‰´ìŠ¤/ë³¸ë¬¸ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ "ë‹¤ìš´ë¡œë“œí•˜ì§€ ë§ê³ " ì´ë¯¸ì§€ URL(src/og:image/a hrefì˜ jpg/png ë“±)ë§Œ ì°¾ì•„ image_urlsì— ë‹´ì•„ë¼.

ì‘ì—… ë‹¨ê³„:
1. URLë¡œ ì´ë™í•œë‹¤.
2. íŒì—…/ì•Œë¦¼ì´ ëœ¨ë©´ ë‹«ëŠ”ë‹¤.
3. íŒíŠ¸ ê²½ë¡œë¥¼ ì°¸ê³ í•´ ìœ ì‚¬í•œ ë²„íŠ¼/íƒ­/ë§í¬ë¥¼ í´ë¦­í•´ë³¸ë‹¤.
4. ì•ˆ ë³´ì´ë©´ '{title}'ì„(ë¥¼) ì‚¬ì´íŠ¸ ë‚´ë¶€ì—ì„œ ê²€ìƒ‰í•˜ê±°ë‚˜ ê´€ë ¨ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•„ íƒìƒ‰í•œë‹¤.
5. ì •ì±…ëª…/ê³µê³ ëª…ì´ '{title}'ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•œë‹¤.
6. ê´€ë ¨ ì„¹ì…˜ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•œ ë’¤ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ë§Œ ì¶œë ¥í•´ë¼. (ì¶”ê°€ ì„¤ëª…/ë¬¸ì¥ ê¸ˆì§€)

{{
  "source_url": "ì‹¤ì œë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•œ í˜ì´ì§€ URL (ì—†ìœ¼ë©´ null)",
  "matched_title": "í˜„ì¬ í˜ì´ì§€ì—ì„œ í™•ì¸í•œ ì •ì±…ëª…/ê³µê³ ëª…/ì œëª© (ì—†ìœ¼ë©´ null)",
  "matched": true,
  "criteria": {{
    "age": "ì—°ë ¹ ìš”ê±´ ìš”ì•½ ë˜ëŠ” 'ì œí•œ ì—†ìŒ'.",
    "region": "ê±°ì£¼ì§€ ìš”ê±´ ìš”ì•½ ë˜ëŠ” 'ì œí•œ ì—†ìŒ'.",
    "income": "ì†Œë“/ì¬ì‚° ê¸°ì¤€ ìš”ì•½ ë˜ëŠ” 'ì œí•œ ì—†ìŒ'.",
    "employment": "ê³ ìš© ìƒíƒœ ê¸°ì¤€ ìš”ì•½ ë˜ëŠ” 'ì œí•œ ì—†ìŒ'.",
    "other": "ê¸°íƒ€ ì£¼ìš” ìê²© ìš”ê±´ í•œ ì¤„ ë˜ëŠ” 'ì—†ìŒ'."
  }},
  "required_documents": ["ì œì¶œì„œë¥˜ë¥¼ í•­ëª© ë¦¬ìŠ¤íŠ¸ë¡œ", "..."],
  "apply_steps": [
    {{"step": 1, "title": "ë‹¨ê³„ ì œëª©", "detail": "ë¬´ì—‡ì„ í•˜ëŠ”ì§€", "url": "í•´ë‹¹ í˜ì´ì§€ê°€ ìˆìœ¼ë©´"}},
    {{"step": 2, "title": "ë‹¨ê³„ ì œëª©", "detail": "ë¬´ì—‡ì„ í•˜ëŠ”ì§€", "url": null}}
  ],
  "apply_channel": "ì˜¨ë¼ì¸/ë°©ë¬¸/ìš°í¸/í˜¼í•© ì¤‘ í•˜ë‚˜ë¡œ ìš”ì•½",
  "apply_period": "ì‹ ì²­ê¸°ê°„/ìƒì‹œ/ë§ˆê°ì¼ ë“± ì›ë¬¸ ê¸°ë°˜ ìš”ì•½",
  "contact": {{"org": "ê¸°ê´€ëª…", "tel": "ì „í™”", "site": "í™ˆí˜ì´ì§€ URL"}},
  "evidence_text": "ê·¼ê±°ê°€ ëœ ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ì—¬ëŸ¬ ì¤„ ì´ì–´ì„œ ë¶™ì—¬ë„£ê¸°.",
  "navigation_path": [
    {{"action":"open","label":"ì‹œì‘ URL","url":"{url}","note":"entry"}},
    {{"action":"click","label":"í´ë¦­í•œ íƒ­/ë²„íŠ¼","url":"ì´ë™í•œ URL","note":"ì™œ í´ë¦­í–ˆëŠ”ì§€"}}
  ],
  "error_message": null,
  "image_urls": ["ì´ë¯¸ì§€ URL(í¬ìŠ¤í„°/ë³¸ë¬¸) ìˆìœ¼ë©´", "..."],
  "confidence": 0.0,
  "needs_review": false
}}
        """.strip()

        if log_callback:
            await log_callback("Playwright ì§€ë¦„ê¸¸(íŒíŠ¸ ê¸°ë°˜) ëª¨ë“œë¡œ ê²€ì¦ ì‹œì‘...")

        return await BrowserService._run_agent(task, entry_url=url, log_callback=log_callback)

    # =========================================
    # REST Deep Trackìš©: ë™ê¸° wrapper (í•„ìˆ˜!)
    # =========================================
    @staticmethod
    def verify_policy_sync(
        policy: Policy,
        navigation_path: Optional[List[Dict[str, Any]]] = None,
        log_callback: Optional[Callable[[str], Any]] = None,
    ) -> Dict[str, Any]:
        """
        PolicyVerificationService.run_verification_job_sync() ì—ì„œ í˜¸ì¶œë˜ëŠ” ë™ê¸° í•¨ìˆ˜.
        ë‚´ë¶€ì—ì„œ asyncio.run()ìœ¼ë¡œ ìœ„ì˜ async í•¨ìˆ˜ë“¤ì„ ì‹¤í–‰í•œë‹¤.
        """

        async def _runner() -> Dict[str, Any]:
            # âœ… navigation_pathê°€ ìˆì–´ë„ "auto-injected openë§Œ ìˆëŠ” ë¹ˆ íŒíŠ¸"ë©´ shortcut ì˜ë¯¸ ì—†ìŒ
            # (ì´ ê²½ìš° shortcut ëª¨ë“œê°€ ì˜¤íˆë ¤ ìƒ›ê¸¸ë¡œ ë¹ ì§ˆ ìˆ˜ ìˆìŒ)
            usable_hint = False
            if navigation_path and isinstance(navigation_path, list):
                # open 1ê°œì§œë¦¬(ì‹œì‘ URLë§Œ)ë©´ ì‹¤ì§ˆ íŒíŠ¸ë¡œ ë³´ì§€ ì•ŠìŒ
                if len(navigation_path) >= 2:
                    usable_hint = True
                elif len(navigation_path) == 1:
                    a0 = navigation_path[0] or {}
                    if (a0.get("action") != "open") or (a0.get("note") != "auto-injected"):
                        usable_hint = True

            if usable_hint:
                return await BrowserService.verify_policy_with_playwright_shortcut(
                    policy,
                    navigation_path,
                    None,
                )
            return await BrowserService.verify_policy_with_agent(policy, None)

        return asyncio.run(_runner())

    # =================================
    # (ì˜µì…˜) ë‚˜ì¤‘ìš©: ê²€ìƒ‰ìš© ë”ë¯¸ í•¨ìˆ˜
    # =================================
    @staticmethod
    async def search_policy_pages_async(
        query: str,
        filters: Dict[str, Any] | None = None,
    ) -> List[Dict[str, Any]]:
        logger.info(
            "[BrowserService] search_policy_pages_async called query=%s, filters=%s",
            query,
            filters,
        )
        return []
