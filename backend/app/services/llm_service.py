#app/services/llm_service.py
from typing import Any, Dict, List, Optional
import logging

import google.generativeai as genai

from app.config import settings
from app.schemas import BadgeStatus, PolicySearchRequest
from app.models import Policy

logger = logging.getLogger(__name__)

if settings.google_api_key:
    genai.configure(api_key=settings.google_api_key)


class LLMService:
    @staticmethod
    def _fallback_summary(
        req: PolicySearchRequest,
        policy: Policy,
    ) -> Dict[str, Any]:
        """
        GOOGLE_API_KEY ë¯¸ì„¤ì • / LLM ì˜¤ë¥˜ ì‹œ ì‚¬ìš©í•˜ëŠ” ê·œì¹™ ê¸°ë°˜ ìš”ì•½.
        - region / category / age ë²”ìœ„ë¥¼ ì¡°í•©í•´ì„œ í•œ ì¤„ ìš”ì•½ ë¬¸ì¥ì„ ë§Œë“ ë‹¤.
        - ë±ƒì§€ëŠ” ì¼ë‹¨ WARNINGìœ¼ë¡œ ê³ ì • (ë‚˜ì¤‘ì— ë£°ì„ ë°”ê¿”ë„ ë¨).
        """
        parts: list[str] = []

        # ì§€ì—­
        if policy.region:
            parts.append(policy.region)

        # ëŒ€ë¶„ë¥˜(ì·¨ì—…Â·ì¼ìë¦¬ / ì†Œë“Â·ìƒí™œ ë“±)
        if policy.category:
            parts.append(policy.category)

        # ë‚˜ì´ ë²”ìœ„
        age_str = None
        if policy.age_min is not None and policy.age_max is not None:
            age_str = f"{policy.age_min}~{policy.age_max}ì„¸"
        elif policy.age_min is not None:
            age_str = f"{policy.age_min}ì„¸ ì´ìƒ"
        elif policy.age_max is not None:
            age_str = f"{policy.age_max}ì„¸ ì´í•˜"

        if age_str:
            parts.append(age_str)

        meta = " Â· ".join(parts) if parts else "ì²­ë…„ ì •ì±…"

        short_summary = f"{meta} ëŒ€ìƒì˜ '{policy.title}' ì§€ì›ì‚¬ì—…ì…ë‹ˆë‹¤."

        return {
            "badge_status": BadgeStatus.WARNING.value,
            "short_summary": short_summary,
            "reason": "LLM ë¹„í™œì„±í™” ìƒíƒœ ë˜ëŠ” í˜¸ì¶œ ì‹¤íŒ¨ (ë£° ê¸°ë°˜ ìš”ì•½)",
            "missing_criteria": [],
        }
    @staticmethod
    def _postprocess_badge(
        parsed: Dict[str, Any],
        policy: Policy,
        req: PolicySearchRequest,
    ) -> Dict[str, Any]:
        """
        Geminiê°€ ì¤€ badge_statusë¥¼ ê·¸ëŒ€ë¡œ ì“°ì§€ ì•Šê³ ,
        ì‚¬ëŒì´ ë³´ê¸° ë” ìì—°ìŠ¤ëŸ½ê²Œ í•œ ë²ˆ ë³´ì •í•˜ëŠ” ë‹¨ê³„.

        - FAILì¸ë° 'ì •ë³´ ë¶€ì¡± / íŒë‹¨ ë¶ˆê°€' ê³„ì—´ì´ë©´ WARNINGìœ¼ë¡œ ì™„í™”
        - ê°•í•œ ë¶€ì • ë¬¸êµ¬ê°€ ì „í˜€ ì—†ëŠ”ë° FAILì´ë©´ WARNINGìœ¼ë¡œ ì™„í™”
        """
        badge = (parsed.get("badge_status") or "").upper()
        reason = (parsed.get("reason") or "").strip()
        summary = (parsed.get("short_summary") or "").strip()

        # ğŸ” 1) ì •ë³´ ë¶€ì¡± / íŒë‹¨ ë¶ˆê°€ íŒ¨í„´ë“¤
        info_lack_keywords = [
            "íŒë‹¨í•  ìˆ˜ ì—†",
            "íŒë‹¨í•˜ê¸° ì–´ë µ",
            "íŒë‹¨í•  ìˆ˜ ì—†ì–´",
            "ì •ë³´ê°€ ì—†ì–´",
            "ì •ë³´ê°€ ë¶€ì¡±",
            "ë¶€ì¡±í•˜ì—¬",
            "ì¶”ê°€ ì •ë³´",
        ]

        # ğŸ” 2) ì§„ì§œ 'ì™„ì „ íƒˆë½' ëŠë‚Œì˜ ê°•í•œ ë¶€ì • íŒ¨í„´ë“¤
        strong_fail_keywords = [
            "ì‹ ì²­í•  ìˆ˜ ì—†",
            "ì‹ ì²­í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ëŒ€ìƒì´ ì•„ë‹™ë‹ˆë‹¤",
            "ì§€ì› ëŒ€ìƒì´ ì•„ë‹™ë‹ˆë‹¤",
            "í•´ë‹¹ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
            "ì œì™¸ë©ë‹ˆë‹¤",
        ]

        # â¶ FAILì¸ ê²½ìš° â†’ ì •ë³´ ë¶€ì¡±ì´ë©´ WARNINGìœ¼ë¡œ ì™„í™”
        if badge == "FAIL":
            # (1) ì •ë³´ ë¶€ì¡± ê³„ì—´ì´ë©´ â†’ WARNINGìœ¼ë¡œ ì™„í™”
            if any(k in reason for k in info_lack_keywords):
                parsed["badge_status"] = BadgeStatus.WARNING.value
                return parsed

            # (2) ê°•í•œ ë¶€ì • ë¬¸êµ¬ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ â†’ WARNINGìœ¼ë¡œ ì™„í™”
            if not any(k in reason for k in strong_fail_keywords):
                parsed["badge_status"] = BadgeStatus.WARNING.value
                return parsed

        # â· WARNINGì¸ë°, ë¬¸êµ¬ê°€ ë„ˆë¬´ ê°•í•˜ê²Œ "ì™„ì „ íƒˆë½"ì´ë©´ â†’ FAILë¡œ ê²©ìƒ
        if badge == "WARNING":
            text_for_check = reason + " " + summary
            if any(k in text_for_check for k in strong_fail_keywords):
                parsed["badge_status"] = BadgeStatus.FAIL.value
                return parsed
        # ë‚˜ë¨¸ì§€ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜
        return parsed
    @staticmethod
    def _build_fast_track_prompt(
        req: PolicySearchRequest,
        policy: Policy,
    ) -> str:
        age_part = f"{req.age}ì„¸" if req.age is not None else "ë‚˜ì´ ì •ë³´ ì—†ìŒ"
        region_part = req.region or "ì§€ì—­ ì •ë³´ ì—†ìŒ"

        # ì •ì±… ì›ë¬¸ í…ìŠ¤íŠ¸ êµ¬ì„±:
        # 1ìˆœìœ„: policy.raw_text
        # 2ìˆœìœ„: raw_snippet + raw_expln + raw_support ì¡°í•©
        text_source = policy.raw_text

        if not text_source:
            pieces: list[str] = []
            for attr in ("raw_snippet", "raw_expln", "raw_support"):
                v = getattr(policy, attr, None)
                if v:
                    pieces.append(str(v))
            text_source = " ".join(pieces)

        text = text_source or ""
        if len(text) > 6000:
            text = text[:6000]

        prompt = (
            "ë‹¤ìŒì€ ì²­ë…„ ì§€ì› ì •ì±…ì˜ ì›ë¬¸ ì¼ë¶€ì´ë‹¤.\n"
            "ì£¼ì–´ì§„ ì‚¬ìš©ìì˜ ë‚˜ì´/ì§€ì—­ì„ ê¸°ì¤€ìœ¼ë¡œ ì´ ì •ì±…ì˜ ì‹ ì²­ ê°€ëŠ¥ì„±ì„ í‰ê°€í•´ë¼.\n\n"
            "ì‘ë‹µ í˜•ì‹ì€ ë°˜ë“œì‹œ JSON í•œ ì¤„ë¡œë§Œ ì¶œë ¥í•œë‹¤.\n"
            '{\n'
            '  "badge_status": "PASS" | "WARNING" | "FAIL",\n'
            '  "short_summary": "í•œ ë¬¸ì¥ ìš”ì•½",\n'
            '  "reason": "íŒë‹¨ ê·¼ê±°",\n'
            '  "missing_criteria": ["ë¶€ì¡±í•œ ì¡°ê±´1", ...]\n'
            "}\n\n"
            f"ì‚¬ìš©ì ë‚˜ì´: {age_part}\n"
            f"ì‚¬ìš©ì ì§€ì—­: {region_part}\n\n"
            f"ì •ì±… ì œëª©: {policy.title}\n"
            f"ì •ì±… ì›ë¬¸(ì¼ë¶€):\n{text}\n"
        )
        return prompt

    @staticmethod
    def _parse_fast_track_result(raw: str) -> Dict[str, Any]:
        import json
        # 0) ì›ë³¸ ë¬¸ìì—´ ì •ë¦¬
        s = raw.strip()

        # 1) ```json ... ``` ê°™ì€ ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ì´ë©´ ë²—ê²¨ë‚´ê¸°
        if s.startswith("```"):
            # ì²« ì¤„(ì˜ˆ: ```json) ì œê±°
            first_newline = s.find("\n")
            if first_newline != -1:
                s = s[first_newline + 1 :]
            # ë§ˆì§€ë§‰ì˜ ``` ì œê±°
            if s.endswith("```"):
                s = s[:-3]
            s = s.strip()

        # 2) í˜¹ì‹œ ì•ë’¤ì— ë‹¤ë¥¸ í…ìŠ¤íŠ¸ê°€ ì„ì—¬ ìˆì–´ë„,
        #    ê°€ì¥ ë°”ê¹¥ ìª½ì˜ { ... } ë²”ìœ„ë§Œ ì˜ë¼ë‚´ê¸°
        if "{" in s and "}" in s:
            start = s.find("{")
            end = s.rfind("}")
            if start != -1 and end != -1 and end > start:
                s = s[start : end + 1]

        try:
            return json.loads(s)
        except Exception as e:
            # ğŸ”¥ ì—¬ê¸°ì„œ LLMì´ ë³´ë‚¸(ì •ë¦¬ëœ) ì›ë¬¸ ì¼ë¶€ë¥¼ ë¡œê·¸ë¡œ ì°ì–´ë³´ì
            logger.warning(
                "[LLMService] FastTrack JSON íŒŒì‹± ì‹¤íŒ¨ (%s): raw=%r",
                e.__class__.__name__,
                s[:300],  # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ
            )
            return {
                "badge_status": "WARNING",
                "short_summary": "ì •ì±… ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "reason": "LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨",
                "missing_criteria": [],
            }

    @staticmethod
    def evaluate_eligibility(
        req: PolicySearchRequest,
        policy: Policy,
    ) -> Dict[str, Any]:
        """
        Fast Trackì—ì„œ ê° ì •ì±…ë³„ë¡œ PASS/WARNING/FAIL ë±ƒì§€ì™€ ìš”ì•½ì„ ë§Œë“œëŠ” í•¨ìˆ˜.
        - GOOGLE_API_KEY ì—†ìœ¼ë©´: ë°”ë¡œ ë”ë¯¸ ê²°ê³¼
        - LLM í˜¸ì¶œ ì¤‘ ì—ëŸ¬ ë‚˜ë©´: ë”ë¯¸ ê²°ê³¼ + ì—ëŸ¬ ì •ë³´ ì‚´ì§ í¬í•¨
        """
        # 1) í‚¤ê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´: ê·œì¹™ ê¸°ë°˜ ê°„ë‹¨ ìš”ì•½
        if not settings.google_api_key:
            logger.warning("[LLMService] GOOGLE_API_KEY ë¯¸ì„¤ì • â†’ fallback ì‚¬ìš©")
            return LLMService._fallback_summary(req, policy)

        prompt = LLMService._build_fast_track_prompt(req, policy)

        try:
            # ğŸ” ì—¬ê¸° ëª¨ë¸ ì´ë¦„ì€ í™˜ê²½ì— ë§ê²Œ ë‚˜ì¤‘ì— ì¡°ì • ê°€ëŠ¥
            logger.info("[LLMService] Gemini FastTrack í˜¸ì¶œ ì‹œì‘ (policy_id=%s)", policy.id)
            model = genai.GenerativeModel("gemini-2.5-flash-lite")
            response = model.generate_content(prompt)
            raw_text = response.text or ""
            logger.debug("[LLMService] Gemini raw ì‘ë‹µ ì¼ë¶€: %r", raw_text[:200])

            parsed = LLMService._parse_fast_track_result(raw_text)
            # ğŸ”§ LLMì´ ì¤€ badgeë¥¼ ì‚¬ëŒì´ ë³´ê¸° ì¢‹ê²Œ í•œ ë²ˆ í›„ì²˜ë¦¬
            parsed = LLMService._postprocess_badge(parsed, policy, req)

            # ğŸ”¥ íŒŒì‹±ì€ ëëŠ”ë°, ì—¬ì „íˆ ìš°ë¦¬ê°€ ì •ì˜í•œ 'ì‹¤íŒ¨ ë©”ì‹œì§€'ë©´ â†’ fallbackìœ¼ë¡œ ëŒ€ì²´
            if parsed.get("short_summary") == "ì •ì±… ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.":
                logger.info(
                    "[LLMService] LLM íŒŒì‹± ê²°ê³¼ê°€ ì‹¤íŒ¨ ê¸°ë³¸ê°’ â†’ fallback ìš”ì•½ ì‚¬ìš© (policy_id=%s)",
                    policy.id,
                )
                return LLMService._fallback_summary(req, policy)

            return parsed

        except Exception as e:
            # LLM í˜¸ì¶œ ì‹¤íŒ¨í•´ë„ FastAPIëŠ” 500 ì•ˆ ë‚´ê³ , ê·œì¹™ ê¸°ë°˜ ìš”ì•½ ì‚¬ìš©
            logger.error(
                "[LLMService] Gemini í˜¸ì¶œ ì˜ˆì™¸ ë°œìƒ â†’ fallback ì‚¬ìš© (%s: %s)",
                e.__class__.__name__,
                str(e),
            )
            fallback = LLMService._fallback_summary(req, policy)
            fallback["reason"] = f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e.__class__.__name__}"
            return fallback

    # ===== Deep Trackì—ì„œ ì‚¬ìš©í•  ì¶”ì¶œìš© =====
    @staticmethod
    def build_deep_track_prompt(page_texts: List[str]) -> str:
        combined = "\n\n---\n\n".join(page_texts)
        if len(combined) > 12000:
            combined = combined[:12000]

        prompt = (
            "ë‹¤ìŒì€ íŠ¹ì • ì²­ë…„ ì •ì±… í˜ì´ì§€ì—ì„œ ìˆ˜ì§‘í•œ í…ìŠ¤íŠ¸ì´ë‹¤.\n"
            "ì´ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ì²­ ìê²©/ëŒ€ìƒ/ê¸°ê°„/í•„ìˆ˜ ì„œë¥˜ ë“± ì¤‘ìš”í•œ ì¡°ê±´ì„ êµ¬ì¡°í™”í•´ë¼.\n\n"
            "ì‘ë‹µ í˜•ì‹ì€ ë°˜ë“œì‹œ JSON í•œ ì¤„ë¡œë§Œ ì¶œë ¥í•œë‹¤.\n"
            "{\n"
            '  "criteria": {\n'
            '    "age": "ì˜ˆ: ë§Œ 19~34ì„¸",\n'
            '    "region": "ì˜ˆ: ì„œìš¸ ê±°ì£¼",\n'
            '    "employment": "ì˜ˆ: ë¯¸ì·¨ì—… ë˜ëŠ” í”„ë¦¬ëœì„œ",\n'
            '    "others": ["ê¸°íƒ€ ì¡°ê±´1", "ê¸°íƒ€ ì¡°ê±´2", ...]\n'
            "  },\n"
            '  "evidence_text": "ê°€ì¥ í•µì‹¬ì ì¸ ë¶€ë¶„ë§Œ ë°œì·Œí•œ ìš”ì•½ í…ìŠ¤íŠ¸"\n'
            "}\n\n"
            "ë‹¤ìŒì€ ìˆ˜ì§‘ëœ ì›ë¬¸ì´ë‹¤:\n"
            f"{combined}\n"
        )
        return prompt

    @staticmethod
    def extract_verification_info(page_texts: List[str]) -> Dict[str, Any]:
        """
        Deep Trackì—ì„œ browser-use/Playwrightê°€ ìˆ˜ì§‘í•œ í…ìŠ¤íŠ¸ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ
        ìê²© ì¡°ê±´/ì¦ë¹™ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”.
        """
        if not settings.google_api_key:
            return {
                "criteria": {
                    "age": "ì •ë³´ ì—†ìŒ",
                    "region": "ì •ë³´ ì—†ìŒ",
                    "employment": "ì •ë³´ ì—†ìŒ",
                    "others": [],
                },
                "evidence_text": "LLM ë¹„í™œì„±í™” ìƒíƒœ (ë”ë¯¸ ë°ì´í„°)",
            }

        prompt = LLMService.build_deep_track_prompt(page_texts)
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(prompt)
        raw = response.text or ""

        import json

        try:
            parsed = json.loads(raw)
        except Exception:
            parsed = {
                "criteria": {
                    "age": "íŒŒì‹± ì‹¤íŒ¨",
                    "region": "íŒŒì‹± ì‹¤íŒ¨",
                    "employment": "íŒŒì‹± ì‹¤íŒ¨",
                    "others": [],
                },
                "evidence_text": "LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨",
            }
        return parsed
